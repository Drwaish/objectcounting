{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCVyYjLXofL9"
      },
      "source": [
        "## Before start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Object Detection, Counting amd Tracking\n",
        "User interface for live and recorded video.\n",
        "\n",
        "## User Interface\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4okzdHlKMaj",
        "outputId": "bbf912e5-63e1-4922-e5ad-bbbaf032fbeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Oct  9 11:21:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "13ytMtH7ZPRE"
      },
      "outputs": [],
      "source": [
        "!pip install ffmpeg -q\n",
        "# !git clone https://github.com/DmytroNorth/Text_To_Subtitles-Python.git\n",
        "\n",
        "!pip install gradio -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "984J4pv4K2D-",
        "outputId": "80e78c51-63dd-4bee-ea0e-b82b8d635419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Getting current working direoctory\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAFba4GkombF"
      },
      "source": [
        "### Install YOLOv8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGckxTNGLKDh",
        "outputId": "65710559-0d4e-4179-8ed5-24f2070ccd6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.195 ðŸš€ Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 27.0/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e1ilpTlovJz"
      },
      "source": [
        "### Install Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KdBkOflo2xY",
        "outputId": "63a2ea0b-225a-4c58-e99d-d2d771a4fc56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yolox.__version__: 0.1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# workaround related to https://github.com/roboflow/notebooks/issues/80\n",
        "!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n",
        "\n",
        "!pip3 install -q -r requirements.txt\n",
        "!python3 setup.py -q develop\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "import yolox\n",
        "print(\"yolox.__version__:\", yolox.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rwg-lY49o7Sf"
      },
      "outputs": [],
      "source": [
        "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
        "from onemetric.cv.utils.iou import box_iou_batch\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BYTETrackerArgs:\n",
        "    track_thresh: float = 0.25\n",
        "    track_buffer: int = 30\n",
        "    match_thresh: float = 0.8\n",
        "    aspect_ratio_thresh: float = 3.0\n",
        "    min_box_area: float = 1.0\n",
        "    mot20: bool = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv2XrHUOo-aJ",
        "outputId": "1e2a4685-590e-45a9-b680-05d056e256d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ByteTrack\n"
          ]
        }
      ],
      "source": [
        "%cd {HOME}/ByteTrack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60yX_PFQ9A2",
        "outputId": "90f2a9ac-eeb3-4f02-fef4-61e366e35d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "supervision.__version__: 0.1.0\n"
          ]
        }
      ],
      "source": [
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import supervision\n",
        "print(\"supervision.__version__:\", supervision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7YDohOpMTWH5"
      },
      "outputs": [],
      "source": [
        "from supervision.draw.color import ColorPalette\n",
        "from supervision.geometry.dataclasses import Point\n",
        "from supervision.video.dataclasses import VideoInfo\n",
        "from supervision.video.source import get_video_frames_generator\n",
        "from supervision.video.sink import VideoSink\n",
        "from supervision.notebook.utils import show_frame_in_notebook\n",
        "from supervision.tools.detections import Detections, BoxAnnotator\n",
        "from supervision.tools.line_counter import LineCounter, LineCounterAnnotator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPdB-v_hWxBy"
      },
      "source": [
        "### Tracking utils\n",
        "\n",
        "Unfortunately, we have to manually match the bounding boxes coming from our model with those created by the tracker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SE0G6LvFAXlk"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# converts Detections into format that can be consumed by match_detections_with_tracks function\n",
        "def detections2boxes(detections: Detections) -> np.ndarray:\n",
        "    return np.hstack((\n",
        "        detections.xyxy,\n",
        "        detections.confidence[:, np.newaxis]\n",
        "    ))\n",
        "\n",
        "\n",
        "# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n",
        "def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n",
        "    return np.array([\n",
        "        track.tlbr\n",
        "        for track\n",
        "        in tracks\n",
        "    ], dtype=float)\n",
        "\n",
        "\n",
        "# matches our bounding boxes with predictions\n",
        "def match_detections_with_tracks(\n",
        "    detections: Detections,\n",
        "    tracks: List[STrack]\n",
        ") -> Detections:\n",
        "    if not np.any(detections.xyxy) or len(tracks) == 0:\n",
        "        return np.empty((0,))\n",
        "\n",
        "    tracks_boxes = tracks2boxes(tracks=tracks)\n",
        "    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n",
        "    track2detection = np.argmax(iou, axis=1)\n",
        "\n",
        "    tracker_ids = [None] * len(detections)\n",
        "\n",
        "    for tracker_index, detection_index in enumerate(track2detection):\n",
        "        if iou[tracker_index, detection_index] != 0:\n",
        "            tracker_ids[detection_index] = tracks[tracker_index].track_id\n",
        "\n",
        "    return tracker_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_417m4g9XVd"
      },
      "source": [
        "### Load pre-trained YOLOv8 model\n",
        "Also available in releases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "m3FMq5FcUsRc"
      },
      "outputs": [],
      "source": [
        "# settings\n",
        "MODEL = \"yolov8x.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFCV_2TR9eo_",
        "outputId": "02696160-f236-4ad4-9a37-317354254d17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients, 257.8 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(MODEL)\n",
        "model.fuse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9aWCMgksKqW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6to6MgPmTnCu"
      },
      "source": [
        "### Predict and annotate Image\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jJQC5brLshjL"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yKuDnOIxsN6l"
      },
      "outputs": [],
      "source": [
        "# dict maping class_id to class_name\n",
        "CLASS_NAMES_DICT = model.model.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hZQsgCa0cFvH"
      },
      "outputs": [],
      "source": [
        "def image_processing(img : Image.Image) -> Image.Image:\n",
        "  \"\"\"\n",
        "    In this method processing will be done on image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img\n",
        "      Image send by iser\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    Image\n",
        "      Return the process image .\n",
        "    \"\"\"\n",
        "  # create instance of BoxAnnotator\n",
        "  box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n",
        "\n",
        "  # model prediction on single frame and conversion to supervision Detections\n",
        "  results = model(img)\n",
        "  detections = Detections(\n",
        "      xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
        "      confidence=results[0].boxes.conf.cpu().numpy(),\n",
        "      class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "  )\n",
        "  # format custom labels\n",
        "  labels = [\n",
        "      f\"{CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
        "      for _, confidence, class_id, tracker_id\n",
        "      in detections\n",
        "  ]\n",
        "  # annotate and display frame\n",
        "  frame = box_annotator.annotate(frame= img, detections=detections, labels=labels)\n",
        "  return frame\n",
        "  # %matplotlib inline\n",
        "  # show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZbGmYfiT0EV"
      },
      "source": [
        "### Predict and annotate whole video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MjP8Pn10XuJm"
      },
      "outputs": [],
      "source": [
        "# settings\n",
        "LINE_START = Point(50, 1500)\n",
        "LINE_END = Point(3840-50, 1500)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2K6ZRC7eSlRb"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mOqJR6MvYDs6"
      },
      "outputs": [],
      "source": [
        "def text_to_srt()-> bool:\n",
        "  \"\"\"\n",
        "  Convert Text File into .srt file for caption\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  None\n",
        "\n",
        "  Return\n",
        "  ------\n",
        "  bool\n",
        "  \"\"\"\n",
        "  flag = os.system(\"python3 text_to_subtitles.py\")\n",
        "  if flag == 0:\n",
        "    return True\n",
        "  return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvRT6UiBcIJu",
        "outputId": "1638ad1d-537f-4cae-e8d1-54a1ad9c3cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(text_to_srt()) # unit testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7GFL0T4HrlNM",
        "outputId": "71e99124-3305-475c-fa45-163fd2af4ab0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/ByteTrack'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "soLUgc24F651"
      },
      "outputs": [],
      "source": [
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "vwFDIWsASvx-"
      },
      "outputs": [],
      "source": [
        "def merget_srt_to_video() -> bool:\n",
        "    \"\"\"\n",
        "    Merge .srt file with video\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    bool\n",
        "    \"\"\"\n",
        "    query = \"ffmpeg -i result.mp4 -vf subtitles=subtitles.srt output.mp4 -y\"\n",
        "    print(query)\n",
        "    var = os.system(query)\n",
        "    print(var)\n",
        "    if var==0:\n",
        "      return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIpsopnCcRQ8",
        "outputId": "2263ee74-7e6b-4d2b-8f64-7b84058fff8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ffmpeg -i result.mp4 -vf subtitles=subtitles.srt output.mp4 -y\n",
            "0\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "print(merget_srt_to_video()) # unit testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1RSpczY8ruFV"
      },
      "outputs": [],
      "source": [
        "def process_video(SOURCE_VIDEO_PATH : str )-> Image.Image:\n",
        "  \"\"\"\n",
        "    In this method video processing will be done.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    SOURCE_VIDEO_PATH\n",
        "      Video to detect track and count object\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    Image\n",
        "      Return the process frame .\n",
        "    \"\"\"\n",
        "  # create BYTETracker instance\n",
        "  byte_tracker = BYTETracker(BYTETrackerArgs())\n",
        "  # create VideoInfo instance\n",
        "  video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "  # create frame generator\n",
        "  generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "  # create LineCounter instance\n",
        "  line_counter = LineCounter(start=LINE_START, end=LINE_END)\n",
        "  # create instance of BoxAnnotator and LineCounterAnnotator\n",
        "  box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n",
        "  line_annotator = LineCounterAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "  print(\"Video Info :\", video_info)\n",
        "  # open target video file\n",
        "      # loop over video frames\n",
        "  for frame in tqdm(generator, total=video_info.total_frames):\n",
        "      # model prediction on single frame and conversion to supervision Detections\n",
        "      results = model(frame)\n",
        "      detections = Detections(\n",
        "          xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
        "          confidence=results[0].boxes.conf.cpu().numpy(),\n",
        "          class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "      )\n",
        "      # filtering out detections with unwanted classes\n",
        "      mask = np.array([class_id for class_id in detections.class_id], dtype=bool)\n",
        "      detections.filter(mask=mask, inplace=True)\n",
        "      # tracking detections\n",
        "      tracks = byte_tracker.update(\n",
        "          output_results=detections2boxes(detections=detections),\n",
        "          img_info=frame.shape,\n",
        "          img_size=frame.shape\n",
        "      )\n",
        "      tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n",
        "      detections.tracker_id = np.array(tracker_id)\n",
        "      # filtering out detections without trackers\n",
        "      mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
        "      detections.filter(mask=mask, inplace=True)\n",
        "      # format custom labels\n",
        "      labels = [\n",
        "          f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n",
        "          for _, confidence, class_id, tracker_id\n",
        "          in detections\n",
        "      ]\n",
        "      with open(\"subtitles.txt\",\"a\") as file:\n",
        "        file.writelines(str(labels) + '\\n' + '\\n')\n",
        "      ids = detections.class_id\n",
        "      for id in ids:\n",
        "        if object_count.count(id) == 0 :\n",
        "          object_count.append(id)\n",
        "      # updating line counter\n",
        "      # line_counter.update(detections=detections)\n",
        "      # annotate and display frame\n",
        "      # line_annotator.annotate(frame=frame, line_counter=line_counter)\n",
        "      yield (box_annotator.annotate(frame=frame, detections=detections, labels=labels), len(object_count))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "VW4X2GUR7qEn"
      },
      "outputs": [],
      "source": [
        "object_count = []\n",
        "def process_live(frame):\n",
        "    \"\"\"\n",
        "    In this method live video processing will be done.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    frame\n",
        "      Image capture by web cam\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    Image\n",
        "      Return the process image realtime.\n",
        "    \"\"\"\n",
        "    byte_tracker = BYTETracker(BYTETrackerArgs())\n",
        "    box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n",
        "\n",
        "    results = model(frame)[0]\n",
        "    detections = Detections(\n",
        "        xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
        "        confidence=results[0].boxes.conf.cpu().numpy(),\n",
        "        class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "    )\n",
        "    # filtering out detections with unwanted classes\n",
        "    mask = np.array([class_id  for class_id in detections.class_id], dtype=bool)\n",
        "    detections.filter(mask=mask, inplace=True)\n",
        "    # tracking detections\n",
        "    tracks = byte_tracker.update(\n",
        "        output_results=detections2boxes(detections=detections),\n",
        "        img_info=frame.shape,\n",
        "        img_size=frame.shape\n",
        "    )\n",
        "    tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n",
        "    detections.tracker_id = np.array(tracker_id)\n",
        "    # filtering out detections without trackers\n",
        "    mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
        "    detections.filter(mask=mask, inplace=True)\n",
        "    # format custom labels\n",
        "    labels = [\n",
        "        f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n",
        "        for _, confidence, class_id, tracker_id\n",
        "        in detections\n",
        "    ]\n",
        "    with open(\"subtitles.txt\",\"a\") as file:\n",
        "        file.writelines(str(labels) + '\\n' + '\\n')\n",
        "    ids = detections.class_id\n",
        "    for id in ids:\n",
        "      object_count.append(id)\n",
        "    # updating line counter\n",
        "    # line_counter.update(detections=detections)\n",
        "    # annotate and display frame\n",
        "    # line_annotator.annotate(frame=frame, line_counter=line_counter)\n",
        "    frame.resize(380, 640)\n",
        "    print(frame.size)\n",
        "    yield (box_annotator.annotate(frame=frame, detections=detections, labels=labels), len(object_count))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "unWAffXKV0xa",
        "outputId": "0b5f4c69-b2a0-46a5-e540-a971f8e499e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/ByteTrack'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "hdxl1s7PbSVO"
      },
      "outputs": [],
      "source": [
        "# Uncooment following lines for if working on colab\n",
        "#from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "FTDWddOgaR3X"
      },
      "outputs": [],
      "source": [
        "def export_video() -> None:\n",
        "  \"\"\"\n",
        "  Download resultant video\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  None\n",
        "\n",
        "  Return\n",
        "  ------\n",
        "  None\n",
        "  \"\"\"\n",
        "  if text_to_srt():\n",
        "    if merget_srt_to_video():\n",
        "      #files.download(\"output.mp4\") # Uncooment on colab\n",
        "      print(\"Caption's file ok\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr3Cft8BA9pX"
      },
      "source": [
        "### Front End"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqLX9tKpsVzA"
      },
      "outputs": [],
      "source": [
        "title = \"Object Detect Tracking and Counting\"\n",
        "with gr.Blocks(theme= gr.themes.Soft()) as io:\n",
        "    with gr.Tab(\"Video Tracking\") as record:\n",
        "        gr.Markdown(f\"<center><h1>{title}</h1></center>\")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                input_image = gr.Video()\n",
        "\n",
        "            with gr.Column():\n",
        "                output_image = gr.Image()\n",
        "\n",
        "        with gr.Row():\n",
        "              total_count = gr.Textbox(label = \"Number of Object\")\n",
        "\n",
        "        with gr.Row():\n",
        "                input_button = gr.Button(\"Start Tracking\")\n",
        "                input_button.click(process_video, inputs=[input_image], outputs=[output_image, total_count])\n",
        "        with gr.Row():\n",
        "                input_button = gr.Button(\"Export Result\")\n",
        "                input_button.click(export_video, inputs= None, outputs = None)\n",
        "\n",
        "    with gr.Tab(\"Live Tracking\") as live:\n",
        "        gr.Markdown(f\"<center><h1>{title}</h1></center>\")\n",
        "        with gr.Row():\n",
        "          with gr.Column():\n",
        "             input_image = gr.Image(source='webcam', streaming=True)\n",
        "\n",
        "          with gr.Column():\n",
        "             output_image = gr.Image()\n",
        "        with gr.Row():\n",
        "              total_count = gr.Textbox(label = \"Number of Object\")\n",
        "        with gr.Row():\n",
        "             button  = gr.Button(\"Start Tracking\")\n",
        "             button.click(process_live, inputs=[input_image], outputs=[output_image])\n",
        "        with gr.Row():\n",
        "                input_button = gr.Button(\"Export Result\")\n",
        "                input_button.click(export_video, inputs= None, outputs = None)\n",
        "io.queue()\n",
        "io.launch(debug = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyGp7EfHEVsA"
      },
      "outputs": [],
      "source": [
        "totalCount = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bg45henTSSI"
      },
      "source": [
        "# FLASK API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_K73JjiTfIct"
      },
      "outputs": [],
      "source": [
        "TARGET_VIDEO_PATH = f\"result.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "QuU7xvb5dmxy"
      },
      "outputs": [],
      "source": [
        "def flask_video(SOURCE_VIDEO_PATH : str ):\n",
        "    \"\"\"\n",
        "    In this method video processing will be done.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    SOURCE_VIDEO_PATH\n",
        "      Video to detect track and count object\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    Image\n",
        "      Return the process frame .\n",
        "    \"\"\"\n",
        "    # if os.path.isdir(TARGET_VIDEO_PATH):\n",
        "\n",
        "    #     os.system(\"rm -rf /content/vid_out\")\n",
        "    #     os.system(\"mkdir vid_out\")\n",
        "      # create BYTETracker instance\n",
        "    tokenize = SOURCE_VIDEO_PATH.split(\".\")\n",
        "    if tokenize[-1] not in (\"mp4\"):\n",
        "      return \"Type Error\"\n",
        "\n",
        "    byte_tracker = BYTETracker(BYTETrackerArgs())\n",
        "    # create VideoInfo instance\n",
        "    video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "    # create frame generator\n",
        "    generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "    # create LineCounter instance\n",
        "    line_counter = LineCounter(start=LINE_START, end=LINE_END)\n",
        "    # create instance of BoxAnnotator and LineCounterAnnotator\n",
        "    box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n",
        "    line_annotator = LineCounterAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "    print(\"Video Info\" , video_info)\n",
        "    # open target video file\n",
        "    with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "        # loop over video frames\n",
        "        for frame in tqdm(generator, total=video_info.total_frames):\n",
        "            # model prediction on single frame and conversion to supervision Detections\n",
        "            results = model(frame)\n",
        "            detections = Detections(\n",
        "                xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
        "                confidence=results[0].boxes.conf.cpu().numpy(),\n",
        "                class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "            )\n",
        "            # filtering out detections with unwanted classes\n",
        "            mask = np.array([class_id  for class_id in detections.class_id], dtype=bool)\n",
        "            detections.filter(mask=mask, inplace=True)\n",
        "            # tracking detections\n",
        "            tracks = byte_tracker.update(\n",
        "                output_results=detections2boxes(detections=detections),\n",
        "                img_info=frame.shape,\n",
        "                img_size=frame.shape\n",
        "            )\n",
        "            tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n",
        "            detections.tracker_id = np.array(tracker_id)\n",
        "            # filtering out detections without trackers\n",
        "            mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
        "            detections.filter(mask=mask, inplace=True)\n",
        "            # format custom labels\n",
        "            labels = [\n",
        "                f\"#{tracker_id} {CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
        "                for _, confidence, class_id, tracker_id\n",
        "                in detections\n",
        "            ]\n",
        "            with open(\"subtitles.txt\",\"a\") as file:\n",
        "              file.writelines(str(labels) + '\\n' + '\\n')\n",
        "            # updating line counter\n",
        "            line_counter.update(detections=detections)\n",
        "            # annotate and display frame\n",
        "            frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)\n",
        "            line_annotator.annotate(frame=frame, line_counter=line_counter)\n",
        "            sink.write_frame(frame)\n",
        "    return \"0k\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZC7IWmngS0V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "trdrWaSwGtyp"
      },
      "outputs": [],
      "source": [
        "def image_processing(path : str) -> None:\n",
        "  \"\"\"\n",
        "    In this method processing will be done on image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path\n",
        "      Path of Image send by user\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    None\n",
        "    \"\"\"\n",
        "  # create instance of BoxAnnotator\n",
        "  box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)\n",
        "  img = Image.open(path)\n",
        "  # model prediction on single frame and conversion to supervision Detections\n",
        "  results = model(img)\n",
        "  detections = Detections(\n",
        "      xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
        "      confidence=results[0].boxes.conf.cpu().numpy(),\n",
        "      class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "  )\n",
        "  # format custom labels\n",
        "  labels = [\n",
        "      f\"{CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
        "      for _, confidence, class_id, tracker_id\n",
        "      in detections\n",
        "  ]\n",
        "  # annotate and display frame\n",
        "  frame = box_annotator.annotate(frame= img, detections=detections, labels=labels)\n",
        "  frame.save(\"temp.jpg\")\n",
        "  # %matplotlib inline\n",
        "  # show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2f3848052562467196d108c37e75a895",
            "9eaa48d31d9d447e8bb386aefaaf94b7",
            "f4e69b36b12e4d268bbf08085eac55f9",
            "bf3c819ad72e44c7ab865a3247944222",
            "8897c46677dd4b5ebec83bb1b67c3706",
            "b91924d5b48247c3a1a72ab58ec55446",
            "4503dd1c079e4ddeb7398d88859b9590",
            "9b92097f60544059903e31c11a8116b0",
            "3cf2f7a171934f16a79500408334fa0e",
            "419f4f9505e8473487e1adf4f66e57e6",
            "1341d1da93cf44bc9b6aed7fc64825a3"
          ]
        },
        "id": "ZIwjk1gf9tMY",
        "outputId": "9a25ac17-b728-4f13-f3c1-2ae8ad1e8148"
      },
      "outputs": [],
      "source": [
        "flask_video(\"traffic.mp4\") # testing either working or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "9QBRTiqhMFn0"
      },
      "outputs": [],
      "source": [
        "from flask_uploads import UploadSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "49lk9HLm_9IJ"
      },
      "outputs": [],
      "source": [
        "from flask import (Flask, abort,\n",
        "                   request, jsonify)\n",
        "from werkzeug.utils import secure_filename\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import gofile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Request Body\n",
        "for video\n",
        "'''\n",
        "{\n",
        "  video : <video>\n",
        "}\n",
        "'''\n",
        "for image\n",
        "'''\n",
        "{\n",
        "   video : <img> \n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "NGROK = os.getenv('NGROK')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1b617857f24642c79c595b8405f66240",
            "bb7b53f8c5234e80ab8c74869925aac4",
            "c8cf24df657f4206b4cc1556a1e00f29",
            "e386fda56c2041af9b52f4f452ba41fb",
            "44e1522b00764f239b83181d0387b4e1",
            "75d9968057364c3180cdee0f15a9dd6c",
            "73203648d7be4121b4e6a70af8ab83e3",
            "341d6c34fcbd4767b33385d7629c2299",
            "5dbb04df87a442a98fcb09b986c84ca6",
            "af0d670c950e4a98aa49057e338f7f64",
            "6f7d0c1d692e46d79455c34a3f3b6ce4"
          ]
        },
        "id": "ijyDkH31w7Mf",
        "outputId": "31da4a3e-4646-46a1-c606-0358e0462cdd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-10-09T12:21:42+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PUBLIC URL https://6b83-35-203-182-44.ngrok-free.app\n",
            " * Serving Flask app '__main__' (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "ERROR:__main__:Exception on /upload [GET]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2073, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1520, in full_dispatch_request\n",
            "    return self.finalize_request(rv)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1539, in finalize_request\n",
            "    response = self.make_response(rv)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1695, in make_response\n",
            "    raise TypeError(\n",
            "TypeError: The view function for 'uploadfile' did not return a valid response. The function either returned None or ended without a return statement.\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Oct/2023 12:24:03] \"\u001b[35m\u001b[1mGET /upload HTTP/1.1\u001b[0m\" 500 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Oct/2023 12:24:04] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File save successfully\n",
            "Video Info VideoInfo(width=1920, height=1080, fps=30, total_frames=99)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b617857f24642c79c595b8405f66240",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/99 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 10 cars, 70.4ms\n",
            "Speed: 3.3ms preprocess, 70.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 traffic light, 69.9ms\n",
            "Speed: 7.8ms preprocess, 69.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 4 trucks, 1 traffic light, 63.6ms\n",
            "Speed: 3.7ms preprocess, 63.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 trucks, 2 traffic lights, 64.1ms\n",
            "Speed: 3.1ms preprocess, 64.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 3 trucks, 1 traffic light, 43.0ms\n",
            "Speed: 4.4ms preprocess, 43.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 4 trucks, 1 traffic light, 42.7ms\n",
            "Speed: 3.1ms preprocess, 42.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 3 trucks, 2 traffic lights, 45.2ms\n",
            "Speed: 3.0ms preprocess, 45.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 4 trucks, 42.7ms\n",
            "Speed: 2.9ms preprocess, 42.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 4 trucks, 42.7ms\n",
            "Speed: 3.0ms preprocess, 42.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 trucks, 43.5ms\n",
            "Speed: 4.3ms preprocess, 43.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 1 traffic light, 42.7ms\n",
            "Speed: 3.9ms preprocess, 42.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 4 trucks, 1 traffic light, 41.1ms\n",
            "Speed: 2.8ms preprocess, 41.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 3 trucks, 1 traffic light, 36.9ms\n",
            "Speed: 6.0ms preprocess, 36.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 36.1ms\n",
            "Speed: 3.2ms preprocess, 36.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 36.1ms\n",
            "Speed: 4.0ms preprocess, 36.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 36.1ms\n",
            "Speed: 3.2ms preprocess, 36.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 3 traffic lights, 37.8ms\n",
            "Speed: 4.6ms preprocess, 37.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 34.4ms\n",
            "Speed: 5.7ms preprocess, 34.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 2 trucks, 2 traffic lights, 34.2ms\n",
            "Speed: 4.2ms preprocess, 34.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 2 traffic lights, 35.5ms\n",
            "Speed: 2.9ms preprocess, 35.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 35.3ms\n",
            "Speed: 3.6ms preprocess, 35.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 2 traffic lights, 34.9ms\n",
            "Speed: 3.6ms preprocess, 34.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 2 traffic lights, 33.8ms\n",
            "Speed: 3.7ms preprocess, 33.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 1 traffic light, 1 stop sign, 32.0ms\n",
            "Speed: 2.7ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 truck, 1 traffic light, 1 stop sign, 31.9ms\n",
            "Speed: 2.7ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 1 stop sign, 32.0ms\n",
            "Speed: 2.2ms preprocess, 32.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 1 stop sign, 32.1ms\n",
            "Speed: 3.0ms preprocess, 32.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 32.3ms\n",
            "Speed: 2.1ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 1 truck, 29.5ms\n",
            "Speed: 3.7ms preprocess, 29.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 2 trucks, 30.5ms\n",
            "Speed: 3.7ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 truck, 29.3ms\n",
            "Speed: 3.7ms preprocess, 29.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 buss, 4 trucks, 1 traffic light, 30.6ms\n",
            "Speed: 3.7ms preprocess, 30.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 2 trucks, 30.0ms\n",
            "Speed: 3.3ms preprocess, 30.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 29.4ms\n",
            "Speed: 2.6ms preprocess, 29.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 3 trucks, 30.3ms\n",
            "Speed: 3.7ms preprocess, 30.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 29.7ms\n",
            "Speed: 3.1ms preprocess, 29.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 trucks, 30.5ms\n",
            "Speed: 3.1ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 4 trucks, 29.7ms\n",
            "Speed: 4.0ms preprocess, 29.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 30.6ms\n",
            "Speed: 2.3ms preprocess, 30.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 29.4ms\n",
            "Speed: 3.2ms preprocess, 29.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 1 traffic light, 30.7ms\n",
            "Speed: 3.7ms preprocess, 30.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 1 traffic light, 29.7ms\n",
            "Speed: 2.1ms preprocess, 29.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 1 traffic light, 29.1ms\n",
            "Speed: 4.0ms preprocess, 29.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 trucks, 1 traffic light, 30.5ms\n",
            "Speed: 3.2ms preprocess, 30.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 trucks, 1 traffic light, 29.2ms\n",
            "Speed: 3.7ms preprocess, 29.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 30.5ms\n",
            "Speed: 3.2ms preprocess, 30.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 3 trucks, 30.2ms\n",
            "Speed: 3.4ms preprocess, 30.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 traffic lights, 30.5ms\n",
            "Speed: 2.8ms preprocess, 30.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 1 traffic light, 30.5ms\n",
            "Speed: 3.2ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 traffic light, 30.0ms\n",
            "Speed: 2.3ms preprocess, 30.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 30.9ms\n",
            "Speed: 2.8ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 1 traffic light, 29.7ms\n",
            "Speed: 3.5ms preprocess, 29.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 1 traffic light, 30.5ms\n",
            "Speed: 3.6ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 30.7ms\n",
            "Speed: 2.8ms preprocess, 30.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 30.1ms\n",
            "Speed: 2.8ms preprocess, 30.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 1 traffic light, 29.4ms\n",
            "Speed: 2.8ms preprocess, 29.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 3 trucks, 29.2ms\n",
            "Speed: 8.9ms preprocess, 29.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 truck, 30.2ms\n",
            "Speed: 3.7ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 trucks, 29.0ms\n",
            "Speed: 3.1ms preprocess, 29.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 4 trucks, 30.6ms\n",
            "Speed: 4.1ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 5 trucks, 30.4ms\n",
            "Speed: 3.9ms preprocess, 30.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 4 trucks, 31.0ms\n",
            "Speed: 3.5ms preprocess, 31.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 3 trucks, 30.8ms\n",
            "Speed: 3.6ms preprocess, 30.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 4 trucks, 30.5ms\n",
            "Speed: 3.4ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 trucks, 30.0ms\n",
            "Speed: 3.6ms preprocess, 30.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 trucks, 30.0ms\n",
            "Speed: 3.6ms preprocess, 30.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 4 trucks, 29.7ms\n",
            "Speed: 3.2ms preprocess, 29.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 trucks, 30.1ms\n",
            "Speed: 3.4ms preprocess, 30.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 31.7ms\n",
            "Speed: 2.9ms preprocess, 31.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 trucks, 30.4ms\n",
            "Speed: 3.4ms preprocess, 30.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 29.4ms\n",
            "Speed: 3.1ms preprocess, 29.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 30.6ms\n",
            "Speed: 2.9ms preprocess, 30.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 29.8ms\n",
            "Speed: 2.8ms preprocess, 29.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 3 trucks, 29.9ms\n",
            "Speed: 3.1ms preprocess, 29.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 truck, 30.5ms\n",
            "Speed: 4.3ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 trucks, 29.6ms\n",
            "Speed: 2.6ms preprocess, 29.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 30.1ms\n",
            "Speed: 3.0ms preprocess, 30.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 3 trucks, 30.6ms\n",
            "Speed: 2.7ms preprocess, 30.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 4 trucks, 29.5ms\n",
            "Speed: 3.0ms preprocess, 29.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 4 trucks, 30.1ms\n",
            "Speed: 2.1ms preprocess, 30.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 4 trucks, 29.8ms\n",
            "Speed: 3.5ms preprocess, 29.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 3 trucks, 31.0ms\n",
            "Speed: 3.0ms preprocess, 31.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 30.1ms\n",
            "Speed: 3.1ms preprocess, 30.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 2 trucks, 30.4ms\n",
            "Speed: 3.7ms preprocess, 30.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 30.2ms\n",
            "Speed: 2.2ms preprocess, 30.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 3 trucks, 29.5ms\n",
            "Speed: 4.0ms preprocess, 29.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 29.6ms\n",
            "Speed: 3.8ms preprocess, 29.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 30.5ms\n",
            "Speed: 3.2ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 6 trucks, 29.7ms\n",
            "Speed: 3.3ms preprocess, 29.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 5 trucks, 30.4ms\n",
            "Speed: 2.8ms preprocess, 30.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 4 trucks, 29.7ms\n",
            "Speed: 2.8ms preprocess, 29.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 30.6ms\n",
            "Speed: 2.7ms preprocess, 30.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 29.4ms\n",
            "Speed: 3.1ms preprocess, 29.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 30.5ms\n",
            "Speed: 3.9ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 29.9ms\n",
            "Speed: 3.8ms preprocess, 29.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 6 trucks, 29.5ms\n",
            "Speed: 2.4ms preprocess, 29.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 trucks, 29.2ms\n",
            "Speed: 4.2ms preprocess, 29.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 trucks, 30.8ms\n",
            "Speed: 2.3ms preprocess, 30.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 1 traffic light, 29.8ms\n",
            "Speed: 4.0ms preprocess, 29.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ffmpeg -i result.mp4 -vf subtitles=subtitles.srt output.mp4 -y\n",
            "0\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b0316a4d-34e1-42e5-add6-fa14612a9c2b\", \"output.mp4\", 2237534)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Oct/2023 12:26:07] \"POST /upload HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "ngrok.set_auth_token(NGROK)\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "app = Flask(__name__)\n",
        "\n",
        "def upload_on_go(path : str)-> str:\n",
        "  \"\"\"\n",
        "  Upload file on go server.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  path\n",
        "    path to upload on server\n",
        "\n",
        "  Return\n",
        "  ------\n",
        "  str\n",
        "    Downloadable link\n",
        "  \"\"\"\n",
        "  server = gofile.getServer()\n",
        "  dict_data = gofile.uploadFile(path)\n",
        "  return dict_data['downloadPage']\n",
        "\n",
        "\n",
        "print(\"PUBLIC URL\", public_url)\n",
        "@app.route('/')\n",
        "def upload_file():\n",
        "   return 'Hello'\n",
        "\n",
        "@app.route('/upload', methods = ['GET', 'POST'])\n",
        "def uploadfile():\n",
        "   if request.method == 'POST': # check if the method is post\n",
        "    if \"video\" in request.files:\n",
        "        f = request.files['video'] # get the file from the files object\n",
        "        path = f.filename\n",
        "        f.save(secure_filename(f.filename)) # this will secure the file\n",
        "        print(\"File save successfully\")\n",
        "        flag = flask_video(path)\n",
        "        if flag==\"0k\":\n",
        "          export_video()\n",
        "          link = upload_on_go(\"output.mp4\")\n",
        "          return jsonify({'Download link': link ,\n",
        "                          'status' : 'Ok'})\n",
        "        else:\n",
        "          return abort(400,jsonify({'Error': 'Type Error'}))\n",
        "\n",
        "    elif \"image\" in request.files:\n",
        "          f = request.files['image'] # get the file from the files object\n",
        "          path = f.filename\n",
        "          f.save(secure_filename(f.filename)) # this will secure the file\n",
        "          print(\"File save successfully\")\n",
        "          image_processing(path)\n",
        "          link = upload_on_go(f\"content/ByteTrack/{TARGET_VIDEO_PATH}\")\n",
        "          return jsonify({'Download link': link ,\n",
        "                          \"type\" : \"JPG\"})\n",
        "    else:\n",
        "        abort(400, msg = \"Bad Params\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   app.run() # running the flask app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll0-vhwZYt-r"
      },
      "outputs": [],
      "source": [
        "# Request Body\n",
        "'''\n",
        "{\n",
        "  video : <video>\n",
        "}\n",
        "'''\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1341d1da93cf44bc9b6aed7fc64825a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b617857f24642c79c595b8405f66240": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb7b53f8c5234e80ab8c74869925aac4",
              "IPY_MODEL_c8cf24df657f4206b4cc1556a1e00f29",
              "IPY_MODEL_e386fda56c2041af9b52f4f452ba41fb"
            ],
            "layout": "IPY_MODEL_44e1522b00764f239b83181d0387b4e1"
          }
        },
        "2f3848052562467196d108c37e75a895": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9eaa48d31d9d447e8bb386aefaaf94b7",
              "IPY_MODEL_f4e69b36b12e4d268bbf08085eac55f9",
              "IPY_MODEL_bf3c819ad72e44c7ab865a3247944222"
            ],
            "layout": "IPY_MODEL_8897c46677dd4b5ebec83bb1b67c3706"
          }
        },
        "341d6c34fcbd4767b33385d7629c2299": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf2f7a171934f16a79500408334fa0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "419f4f9505e8473487e1adf4f66e57e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44e1522b00764f239b83181d0387b4e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4503dd1c079e4ddeb7398d88859b9590": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dbb04df87a442a98fcb09b986c84ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f7d0c1d692e46d79455c34a3f3b6ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73203648d7be4121b4e6a70af8ab83e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75d9968057364c3180cdee0f15a9dd6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8897c46677dd4b5ebec83bb1b67c3706": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b92097f60544059903e31c11a8116b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eaa48d31d9d447e8bb386aefaaf94b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b91924d5b48247c3a1a72ab58ec55446",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4503dd1c079e4ddeb7398d88859b9590",
            "value": "100%"
          }
        },
        "af0d670c950e4a98aa49057e338f7f64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91924d5b48247c3a1a72ab58ec55446": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7b53f8c5234e80ab8c74869925aac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75d9968057364c3180cdee0f15a9dd6c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_73203648d7be4121b4e6a70af8ab83e3",
            "value": "100%"
          }
        },
        "bf3c819ad72e44c7ab865a3247944222": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_419f4f9505e8473487e1adf4f66e57e6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1341d1da93cf44bc9b6aed7fc64825a3",
            "value": " 317/318 [00:17&lt;00:00, 17.13it/s]"
          }
        },
        "c8cf24df657f4206b4cc1556a1e00f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_341d6c34fcbd4767b33385d7629c2299",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dbb04df87a442a98fcb09b986c84ca6",
            "value": 99
          }
        },
        "e386fda56c2041af9b52f4f452ba41fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af0d670c950e4a98aa49057e338f7f64",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6f7d0c1d692e46d79455c34a3f3b6ce4",
            "value": " 99/99 [00:08&lt;00:00, 13.48it/s]"
          }
        },
        "f4e69b36b12e4d268bbf08085eac55f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b92097f60544059903e31c11a8116b0",
            "max": 318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cf2f7a171934f16a79500408334fa0e",
            "value": 317
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
