{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWeDcchFYGC_"
      },
      "source": [
        "# Live Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t-afOulYnJRh"
      },
      "outputs": [],
      "source": [
        "!pip install gradio -q\n",
        "!pip install ultralytics -q\n",
        "!pip install supervision -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uqwWeN56IDFo"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import gradio as gr\n",
        "import supervision as sv\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import cv2  # for video to image conversion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AFWM1RlqK9Ax"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFCgkipAK2zF"
      },
      "outputs": [],
      "source": [
        "def text_to_srt()-> bool:\n",
        "  \"\"\"\n",
        "  Convert Text File into .srt file for caption\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  None\n",
        "\n",
        "  Return\n",
        "  ------\n",
        "  bool\n",
        "  \"\"\"\n",
        "  flag = os.system(\"python3 text_to_subtitles.py\")\n",
        "  if flag == 0:\n",
        "    return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RdtixB8K-AO"
      },
      "outputs": [],
      "source": [
        "def merget_srt_to_video() -> bool:\n",
        "    \"\"\"\n",
        "    Merge .srt file with video\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    bool\n",
        "    \"\"\"\n",
        "    query = \"ffmpeg -i result.mp4 -vf subtitles=subtitles.srt output.mp4 -y\"\n",
        "    print(query)\n",
        "    var = os.system(query)\n",
        "    print(var)\n",
        "    if var==0:\n",
        "      return True\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "169eYVVnLJgs"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dXfF656LGaC"
      },
      "outputs": [],
      "source": [
        "def export_video() -> None:\n",
        "  \"\"\"\n",
        "  Download resultant video\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  None\n",
        "\n",
        "  Return\n",
        "  ------\n",
        "  None\n",
        "  \"\"\"\n",
        "  if text_to_srt():\n",
        "    if merget_srt_to_video():\n",
        "      files.download(\"output.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wiud_BhFKrBa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model = YOLO('yolov8s.pt')\n",
        "byte_tracker = sv.ByteTrack()\n",
        "annotator = sv.BoxAnnotator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtGldhfpHJdN"
      },
      "outputs": [],
      "source": [
        "def process_live(frame):\n",
        "    results = model(frame)[0]\n",
        "    detections = sv.Detections.from_ultralytics(results)\n",
        "    annotator = sv.BoxAnnotator()\n",
        "    detections = byte_tracker.update_with_detections(detections)\n",
        "    labels = [\n",
        "         f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n",
        "         for _, _, confidence, class_id, tracker_id\n",
        "         in detections\n",
        "    ]\n",
        "    yield  annotator.annotate(scene=frame.copy(),\n",
        "                               detections=detections, labels=labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc81fs3fHmc8"
      },
      "outputs": [],
      "source": [
        "def process_video(SOURCE_VIDEO_PATH):\n",
        "      \"\"\"\n",
        "      In this method video processing will be done.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      SOURCE_VIDEO_PATH\n",
        "        Video to detect track and count object\n",
        "\n",
        "      Return\n",
        "      ------\n",
        "      Image\n",
        "        Return the process frame .\n",
        "      \"\"\"\n",
        "      # create BYTETracker instance\n",
        "      byte_tracker = sv.ByteTrack()\n",
        "\n",
        "      video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "      # create frame generator\n",
        "      generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "      # create LineCounter instance\n",
        "      # create instance of BoxAnnotator and LineCounterAnnotator\n",
        "      box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "      print(\"Video Info :\", video_info)\n",
        "      # open target video file\n",
        "          # loop over video frames\n",
        "      for frame in tqdm(generator, total=video_info.total_frames):\n",
        "          # model prediction on single frame and conversion to supervision Detections\n",
        "        results = model(frame)\n",
        "        # detections = sv.Detections.from_ultralytics(results)\n",
        "        detections = sv.Detections(\n",
        "          xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
        "          confidence=results[0].boxes.conf.cpu().numpy(),\n",
        "          class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "      )\n",
        "        detections = byte_tracker.update_with_detections(detections)\n",
        "        labels = [\n",
        "            f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n",
        "            for _, _, confidence, class_id, tracker_id\n",
        "            in detections\n",
        "        ]\n",
        "      with open(\"subtitles.txt\",\"a\") as file:\n",
        "        file.writelines(str(labels) + '\\n' + '\\n')\n",
        "      ids = detections.class_id\n",
        "      for id in ids:\n",
        "        if object_count.count(id) == 0 :\n",
        "          object_count.append(id)\n",
        "      yield (box_annotator.annotate(frame=frame, detections=detections, labels=labels), len(object_count))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6ffcb703d91542d8b23436c261a4c08e",
            "349dffa168ef4ce4bf0822f0e31d75ce",
            "4c38c34c12694b6886f430451b15eb46",
            "360d8b86050b40ed8015437643a3cc15",
            "5daefe68fc3b4669ab04d2291e9e344e",
            "d6fa9af8e8d1440ab17a00ef914bf561",
            "d7ac195132624d91b7c7fa75db1673de",
            "30c06c90e5914378bdc4e5d1a9bf456b",
            "ff2da230896347ed87e9e253e2c7fe38",
            "c50c42e25e80435aa350d21ad54e5335",
            "c3853113ebab45b786de8638fa0ef146"
          ]
        },
        "id": "XmCtSavfXnS9",
        "outputId": "fcf59e82-67d9-4bf2-dbd4-252c819c886e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://89a2d90d5a2b171aed.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://89a2d90d5a2b171aed.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video Info : VideoInfo(width=640, height=360, fps=30, total_frames=318)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ffcb703d91542d8b23436c261a4c08e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/318 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 16.2ms\n",
            "Speed: 3.9ms preprocess, 16.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 11.2ms\n",
            "Speed: 2.2ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 11.8ms\n",
            "Speed: 1.8ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 11.2ms\n",
            "Speed: 1.9ms preprocess, 11.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 11.2ms\n",
            "Speed: 1.9ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 11.2ms\n",
            "Speed: 1.9ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 11.2ms\n",
            "Speed: 1.2ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 11.6ms\n",
            "Speed: 1.3ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 umbrella, 12.4ms\n",
            "Speed: 1.2ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 11.2ms\n",
            "Speed: 1.9ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 11.8ms\n",
            "Speed: 1.5ms preprocess, 11.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 umbrella, 11.2ms\n",
            "Speed: 2.2ms preprocess, 11.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 umbrella, 11.2ms\n",
            "Speed: 1.3ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 11.4ms\n",
            "Speed: 1.7ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 11.2ms\n",
            "Speed: 1.4ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 1 umbrella, 10.9ms\n",
            "Speed: 2.6ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 11.6ms\n",
            "Speed: 1.5ms preprocess, 11.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 1 umbrella, 11.4ms\n",
            "Speed: 1.3ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 1 umbrella, 11.0ms\n",
            "Speed: 1.2ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 1 umbrella, 10.5ms\n",
            "Speed: 1.5ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 1 umbrella, 11.7ms\n",
            "Speed: 1.4ms preprocess, 11.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 1 umbrella, 10.5ms\n",
            "Speed: 1.7ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 trucks, 1 umbrella, 12.1ms\n",
            "Speed: 1.2ms preprocess, 12.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 trucks, 1 umbrella, 10.4ms\n",
            "Speed: 1.6ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 1 umbrella, 10.1ms\n",
            "Speed: 2.1ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 truck, 1 umbrella, 9.8ms\n",
            "Speed: 1.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 15.3ms\n",
            "Speed: 1.5ms preprocess, 15.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 10.0ms\n",
            "Speed: 1.4ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 9.9ms\n",
            "Speed: 2.2ms preprocess, 9.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 9.7ms\n",
            "Speed: 1.7ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 10.6ms\n",
            "Speed: 2.1ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 10.3ms\n",
            "Speed: 1.3ms preprocess, 10.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 10.6ms\n",
            "Speed: 1.1ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 11.1ms\n",
            "Speed: 1.5ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 9.7ms\n",
            "Speed: 2.6ms preprocess, 9.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 9.7ms\n",
            "Speed: 1.0ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 10.6ms\n",
            "Speed: 1.3ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 9.7ms\n",
            "Speed: 2.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 1 umbrella, 10.3ms\n",
            "Speed: 1.4ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 9.7ms\n",
            "Speed: 0.9ms preprocess, 9.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 12 cars, 1 umbrella, 10.5ms\n",
            "Speed: 1.3ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 11 cars, 1 truck, 1 umbrella, 13.4ms\n",
            "Speed: 1.5ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 11 cars, 1 truck, 1 umbrella, 10.4ms\n",
            "Speed: 1.9ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 12 cars, 2 trucks, 1 umbrella, 11.1ms\n",
            "Speed: 1.4ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 12 cars, 2 trucks, 1 umbrella, 9.8ms\n",
            "Speed: 2.0ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 9.8ms\n",
            "Speed: 1.2ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 11.0ms\n",
            "Speed: 1.0ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 10.0ms\n",
            "Speed: 1.1ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 10.0ms\n",
            "Speed: 1.1ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 10.4ms\n",
            "Speed: 1.3ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 9.7ms\n",
            "Speed: 1.1ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 9.8ms\n",
            "Speed: 1.3ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 9.7ms\n",
            "Speed: 1.7ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.4ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 10.7ms\n",
            "Speed: 2.1ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 11.5ms\n",
            "Speed: 2.4ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.9ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 10.0ms\n",
            "Speed: 1.5ms preprocess, 10.0ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 umbrella, 10.7ms\n",
            "Speed: 1.4ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 9.7ms\n",
            "Speed: 1.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 9.6ms\n",
            "Speed: 4.0ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 9.5ms\n",
            "Speed: 1.8ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 9.5ms\n",
            "Speed: 1.8ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 1 umbrella, 10.2ms\n",
            "Speed: 3.0ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 13 cars, 1 umbrella, 9.5ms\n",
            "Speed: 2.0ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.7ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.6ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 17.9ms\n",
            "Speed: 1.4ms preprocess, 17.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.7ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 9.6ms\n",
            "Speed: 2.3ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.5ms preprocess, 9.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 10.0ms\n",
            "Speed: 1.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 umbrella, 13.3ms\n",
            "Speed: 2.9ms preprocess, 13.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 14 cars, 1 umbrella, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 10.9ms\n",
            "Speed: 1.3ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1 umbrella, 12.4ms\n",
            "Speed: 1.3ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 umbrella, 9.8ms\n",
            "Speed: 1.4ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 truck, 1 umbrella, 9.6ms\n",
            "Speed: 1.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 12 cars, 1 truck, 1 umbrella, 9.6ms\n",
            "Speed: 1.3ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 truck, 1 umbrella, 9.6ms\n",
            "Speed: 1.2ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 9.6ms\n",
            "Speed: 2.0ms preprocess, 9.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1 umbrella, 10.2ms\n",
            "Speed: 1.3ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 12 cars, 1 truck, 1 umbrella, 9.7ms\n",
            "Speed: 4.6ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1 umbrella, 11.0ms\n",
            "Speed: 1.3ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 truck, 1 umbrella, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 trucks, 1 umbrella, 9.6ms\n",
            "Speed: 1.7ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 trucks, 1 umbrella, 1 frisbee, 10.3ms\n",
            "Speed: 2.6ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 trucks, 1 umbrella, 1 frisbee, 12.2ms\n",
            "Speed: 2.8ms preprocess, 12.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 truck, 1 umbrella, 1 frisbee, 10.3ms\n",
            "Speed: 1.2ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 truck, 1 umbrella, 1 frisbee, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 truck, 1 umbrella, 1 frisbee, 9.5ms\n",
            "Speed: 1.8ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 2 trucks, 1 umbrella, 1 frisbee, 10.6ms\n",
            "Speed: 1.4ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 truck, 1 umbrella, 1 frisbee, 9.6ms\n",
            "Speed: 1.4ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 2 trucks, 1 umbrella, 1 frisbee, 10.1ms\n",
            "Speed: 1.2ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 trucks, 1 umbrella, 1 frisbee, 9.6ms\n",
            "Speed: 1.7ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 trucks, 1 umbrella, 1 frisbee, 10.1ms\n",
            "Speed: 2.3ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 trucks, 1 umbrella, 1 frisbee, 9.5ms\n",
            "Speed: 1.3ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 3 trucks, 1 umbrella, 1 frisbee, 10.3ms\n",
            "Speed: 1.3ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 truck, 1 umbrella, 1 frisbee, 10.4ms\n",
            "Speed: 1.4ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 truck, 1 umbrella, 1 frisbee, 10.5ms\n",
            "Speed: 1.9ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 bus, 1 truck, 1 umbrella, 1 frisbee, 10.4ms\n",
            "Speed: 1.3ms preprocess, 10.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 bus, 1 truck, 1 umbrella, 1 frisbee, 9.6ms\n",
            "Speed: 1.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 truck, 1 umbrella, 1 frisbee, 10.5ms\n",
            "Speed: 1.5ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 truck, 1 umbrella, 1 frisbee, 10.2ms\n",
            "Speed: 1.4ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 2 trucks, 1 umbrella, 9.6ms\n",
            "Speed: 1.4ms preprocess, 9.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 trucks, 1 umbrella, 9.6ms\n",
            "Speed: 2.5ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 truck, 1 umbrella, 10.0ms\n",
            "Speed: 2.0ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1 umbrella, 11.3ms\n",
            "Speed: 1.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1 umbrella, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1 umbrella, 9.5ms\n",
            "Speed: 1.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 2 trucks, 1 umbrella, 12.1ms\n",
            "Speed: 1.3ms preprocess, 12.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 1 truck, 1 umbrella, 10.2ms\n",
            "Speed: 1.3ms preprocess, 10.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 1 truck, 1 umbrella, 9.7ms\n",
            "Speed: 2.2ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1 umbrella, 1 frisbee, 9.6ms\n",
            "Speed: 1.6ms preprocess, 9.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 2 trucks, 1 umbrella, 17.0ms\n",
            "Speed: 1.2ms preprocess, 17.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 11 cars, 1 bus, 1 truck, 1 umbrella, 10.4ms\n",
            "Speed: 1.3ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 trucks, 1 umbrella, 9.6ms\n",
            "Speed: 1.4ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 trucks, 1 umbrella, 9.6ms\n",
            "Speed: 1.2ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 bus, 2 trucks, 1 umbrella, 12.3ms\n",
            "Speed: 1.8ms preprocess, 12.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 12 cars, 1 bus, 2 trucks, 1 frisbee, 9.7ms\n",
            "Speed: 2.7ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 bus, 1 truck, 1 umbrella, 10.5ms\n",
            "Speed: 1.2ms preprocess, 10.5ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 buss, 1 umbrella, 10.0ms\n",
            "Speed: 1.2ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 buss, 1 umbrella, 10.6ms\n",
            "Speed: 4.2ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 buss, 1 umbrella, 9.7ms\n",
            "Speed: 1.0ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 buss, 1 umbrella, 10.5ms\n",
            "Speed: 1.4ms preprocess, 10.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 buss, 1 umbrella, 9.8ms\n",
            "Speed: 1.3ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 buss, 1 umbrella, 9.7ms\n",
            "Speed: 5.8ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 buss, 1 umbrella, 14.0ms\n",
            "Speed: 1.6ms preprocess, 14.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 1 umbrella, 9.7ms\n",
            "Speed: 2.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 2 buss, 1 umbrella, 9.7ms\n",
            "Speed: 0.9ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 1 umbrella, 9.9ms\n",
            "Speed: 1.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 1 umbrella, 12.0ms\n",
            "Speed: 1.4ms preprocess, 12.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 1 umbrella, 10.6ms\n",
            "Speed: 1.4ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 1 umbrella, 11.1ms\n",
            "Speed: 1.0ms preprocess, 11.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 1 umbrella, 9.9ms\n",
            "Speed: 1.1ms preprocess, 9.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 buss, 1 umbrella, 10.7ms\n",
            "Speed: 2.4ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 1 umbrella, 10.6ms\n",
            "Speed: 1.2ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 1 truck, 1 umbrella, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 1 truck, 1 umbrella, 10.5ms\n",
            "Speed: 1.1ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 bus, 1 truck, 1 umbrella, 11.5ms\n",
            "Speed: 1.3ms preprocess, 11.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 1 bus, 1 truck, 1 boat, 1 umbrella, 1 frisbee, 9.9ms\n",
            "Speed: 1.4ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 1 truck, 1 boat, 1 umbrella, 1 frisbee, 9.9ms\n",
            "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 1 umbrella, 1 frisbee, 11.9ms\n",
            "Speed: 1.2ms preprocess, 11.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 1 umbrella, 1 frisbee, 10.9ms\n",
            "Speed: 1.1ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 1 frisbee, 9.9ms\n",
            "Speed: 1.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 1 frisbee, 9.9ms\n",
            "Speed: 1.4ms preprocess, 9.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 1 frisbee, 9.9ms\n",
            "Speed: 1.5ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 14.4ms\n",
            "Speed: 1.1ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 11.9ms\n",
            "Speed: 1.4ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 1 frisbee, 10.1ms\n",
            "Speed: 1.8ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 umbrella, 1 frisbee, 9.9ms\n",
            "Speed: 1.3ms preprocess, 9.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 10.2ms\n",
            "Speed: 1.8ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 10.2ms\n",
            "Speed: 1.7ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 10.0ms\n",
            "Speed: 1.7ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 9.9ms\n",
            "Speed: 1.3ms preprocess, 9.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 10.8ms\n",
            "Speed: 1.2ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 10.8ms\n",
            "Speed: 1.2ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 10.2ms\n",
            "Speed: 1.1ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 10.0ms\n",
            "Speed: 3.1ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 1 frisbee, 11.7ms\n",
            "Speed: 1.1ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 9.9ms\n",
            "Speed: 1.1ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 9.9ms\n",
            "Speed: 1.9ms preprocess, 9.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 12.0ms\n",
            "Speed: 1.4ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 11.4ms\n",
            "Speed: 1.3ms preprocess, 11.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 9.9ms\n",
            "Speed: 1.6ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 9.9ms\n",
            "Speed: 0.9ms preprocess, 9.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 10.2ms\n",
            "Speed: 1.2ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 10.0ms\n",
            "Speed: 1.9ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 9.9ms\n",
            "Speed: 1.1ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 1 truck, 9.9ms\n",
            "Speed: 1.9ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 11 cars, 10.0ms\n",
            "Speed: 1.3ms preprocess, 10.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 11.3ms\n",
            "Speed: 2.7ms preprocess, 11.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 10.9ms\n",
            "Speed: 1.1ms preprocess, 10.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 10.0ms\n",
            "Speed: 1.3ms preprocess, 10.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 9.9ms\n",
            "Speed: 1.3ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 11.2ms\n",
            "Speed: 1.2ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 9.9ms\n",
            "Speed: 1.2ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 9.9ms\n",
            "Speed: 3.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 11.1ms\n",
            "Speed: 1.2ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 14.5ms\n",
            "Speed: 1.1ms preprocess, 14.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 1 truck, 10.3ms\n",
            "Speed: 1.1ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 9.7ms\n",
            "Speed: 1.9ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 truck, 9.8ms\n",
            "Speed: 1.4ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 truck, 13.7ms\n",
            "Speed: 1.1ms preprocess, 13.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 9.8ms\n",
            "Speed: 1.1ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 9.9ms\n",
            "Speed: 1.8ms preprocess, 9.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 9.7ms\n",
            "Speed: 1.2ms preprocess, 9.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 9.7ms\n",
            "Speed: 1.7ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 10.9ms\n",
            "Speed: 1.3ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 9.6ms\n",
            "Speed: 1.8ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 cars, 9.5ms\n",
            "Speed: 1.9ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 11 cars, 9.8ms\n",
            "Speed: 0.9ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 9.6ms\n",
            "Speed: 1.6ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 11.7ms\n",
            "Speed: 1.5ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 9.8ms\n",
            "Speed: 1.5ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 9.7ms\n",
            "Speed: 1.2ms preprocess, 9.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 10.9ms\n",
            "Speed: 1.1ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 cars, 14.8ms\n",
            "Speed: 1.1ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 12.7ms\n",
            "Speed: 1.5ms preprocess, 12.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 11.1ms\n",
            "Speed: 1.1ms preprocess, 11.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 10.4ms\n",
            "Speed: 1.3ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 13.8ms\n",
            "Speed: 1.1ms preprocess, 13.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 11.5ms\n",
            "Speed: 2.0ms preprocess, 11.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.1ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 11.4ms\n",
            "Speed: 1.1ms preprocess, 11.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 umbrella, 13.5ms\n",
            "Speed: 1.2ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 umbrella, 12.6ms\n",
            "Speed: 1.1ms preprocess, 12.6ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 9.7ms\n",
            "Speed: 1.1ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 1 umbrella, 11.3ms\n",
            "Speed: 1.1ms preprocess, 11.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 10.5ms\n",
            "Speed: 1.2ms preprocess, 10.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.3ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.1ms preprocess, 9.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 10.1ms\n",
            "Speed: 2.3ms preprocess, 10.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 13.4ms\n",
            "Speed: 1.4ms preprocess, 13.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 9.9ms\n",
            "Speed: 1.2ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 10.9ms\n",
            "Speed: 1.1ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 9.6ms\n",
            "Speed: 1.2ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 16.3ms\n",
            "Speed: 1.6ms preprocess, 16.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 9.6ms\n",
            "Speed: 1.1ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 9.5ms\n",
            "Speed: 1.2ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 9.7ms\n",
            "Speed: 1.3ms preprocess, 9.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 1 umbrella, 13.0ms\n",
            "Speed: 1.1ms preprocess, 13.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 umbrella, 12.1ms\n",
            "Speed: 1.1ms preprocess, 12.1ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 15.9ms\n",
            "Speed: 1.9ms preprocess, 15.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 truck, 1 umbrella, 11.2ms\n",
            "Speed: 1.6ms preprocess, 11.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 1 umbrella, 10.3ms\n",
            "Speed: 2.6ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 10.4ms\n",
            "Speed: 1.1ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 umbrella, 11.2ms\n",
            "Speed: 1.1ms preprocess, 11.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 umbrella, 17.0ms\n",
            "Speed: 1.1ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 13.8ms\n",
            "Speed: 1.6ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 umbrella, 10.0ms\n",
            "Speed: 1.2ms preprocess, 10.0ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 umbrella, 9.6ms\n",
            "Speed: 1.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 umbrella, 11.6ms\n",
            "Speed: 1.2ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 umbrella, 11.2ms\n",
            "Speed: 1.2ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 umbrella, 10.2ms\n",
            "Speed: 1.2ms preprocess, 10.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 umbrella, 9.6ms\n",
            "Speed: 1.1ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 13.5ms\n",
            "Speed: 1.2ms preprocess, 13.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 9.6ms\n",
            "Speed: 1.2ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 umbrella, 9.9ms\n",
            "Speed: 1.4ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 9.6ms\n",
            "Speed: 1.1ms preprocess, 9.6ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 13.6ms\n",
            "Speed: 3.0ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 15.2ms\n",
            "Speed: 1.2ms preprocess, 15.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 12.0ms\n",
            "Speed: 1.1ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 11.0ms\n",
            "Speed: 1.1ms preprocess, 11.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 14.0ms\n",
            "Speed: 1.3ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 10.0ms\n",
            "Speed: 1.2ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 11.0ms\n",
            "Speed: 1.1ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 9.8ms\n",
            "Speed: 1.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 15.3ms\n",
            "Speed: 1.2ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 11.1ms\n",
            "Speed: 1.1ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 9.7ms\n",
            "Speed: 1.1ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 12.3ms\n",
            "Speed: 1.1ms preprocess, 12.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 14.4ms\n",
            "Speed: 1.0ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 11.8ms\n",
            "Speed: 2.9ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 11.4ms\n",
            "Speed: 2.8ms preprocess, 11.4ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 umbrella, 10.5ms\n",
            "Speed: 1.2ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 umbrella, 11.5ms\n",
            "Speed: 1.1ms preprocess, 11.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 umbrella, 10.4ms\n",
            "Speed: 1.2ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 umbrella, 13.8ms\n",
            "Speed: 1.1ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 umbrella, 11.4ms\n",
            "Speed: 1.2ms preprocess, 11.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 12.7ms\n",
            "Speed: 1.2ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 10.1ms\n",
            "Speed: 1.4ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 10.2ms\n",
            "Speed: 1.1ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 12.9ms\n",
            "Speed: 1.5ms preprocess, 12.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 1 umbrella, 13.3ms\n",
            "Speed: 1.1ms preprocess, 13.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 1 umbrella, 10.6ms\n",
            "Speed: 1.3ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 train, 1 umbrella, 10.1ms\n",
            "Speed: 1.2ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 train, 1 umbrella, 10.3ms\n",
            "Speed: 1.3ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 1 umbrella, 10.1ms\n",
            "Speed: 1.1ms preprocess, 10.1ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 train, 2 trucks, 1 umbrella, 11.6ms\n",
            "Speed: 2.9ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 train, 1 truck, 1 umbrella, 10.1ms\n",
            "Speed: 1.2ms preprocess, 10.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 umbrella, 12.8ms\n",
            "Speed: 1.1ms preprocess, 12.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 umbrella, 13.3ms\n",
            "Speed: 1.1ms preprocess, 13.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 10.2ms\n",
            "Speed: 2.2ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 umbrella, 10.1ms\n",
            "Speed: 1.2ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 11.5ms\n",
            "Speed: 1.2ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 train, 1 truck, 1 umbrella, 11.8ms\n",
            "Speed: 1.3ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 1 umbrella, 11.2ms\n",
            "Speed: 1.2ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 1 umbrella, 14.1ms\n",
            "Speed: 1.2ms preprocess, 14.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 14.8ms\n",
            "Speed: 1.4ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 15.5ms\n",
            "Speed: 1.2ms preprocess, 15.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 11.5ms\n",
            "Speed: 1.2ms preprocess, 11.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 10.3ms\n",
            "Speed: 1.1ms preprocess, 10.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 10.8ms\n",
            "Speed: 1.2ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 umbrella, 10.3ms\n",
            "Speed: 3.5ms preprocess, 10.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 umbrella, 11.5ms\n",
            "Speed: 1.1ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 1 umbrella, 10.5ms\n",
            "Speed: 1.2ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 10.3ms\n",
            "Speed: 1.2ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 11.5ms\n",
            "Speed: 1.1ms preprocess, 11.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 16.5ms\n",
            "Speed: 1.3ms preprocess, 16.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 1 umbrella, 13.0ms\n",
            "Speed: 1.2ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 1 umbrella, 12.5ms\n",
            "Speed: 1.2ms preprocess, 12.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 1 umbrella, 12.5ms\n",
            "Speed: 1.3ms preprocess, 12.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 1 umbrella, 10.6ms\n",
            "Speed: 1.2ms preprocess, 10.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 truck, 1 umbrella, 10.5ms\n",
            "Speed: 1.2ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 10.7ms\n",
            "Speed: 1.1ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 1 umbrella, 12.5ms\n",
            "Speed: 1.3ms preprocess, 12.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 10.9ms\n",
            "Speed: 1.2ms preprocess, 10.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 20.7ms\n",
            "Speed: 1.2ms preprocess, 20.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 cars, 1 truck, 1 umbrella, 18.1ms\n",
            "Speed: 1.3ms preprocess, 18.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 13.0ms\n",
            "Speed: 1.9ms preprocess, 13.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 11.3ms\n",
            "Speed: 1.5ms preprocess, 11.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 14.2ms\n",
            "Speed: 1.2ms preprocess, 14.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 15.4ms\n",
            "Speed: 1.1ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 14.2ms\n",
            "Speed: 1.2ms preprocess, 14.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 14.5ms\n",
            "Speed: 1.1ms preprocess, 14.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 11.9ms\n",
            "Speed: 1.2ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 12.5ms\n",
            "Speed: 1.4ms preprocess, 12.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 1 umbrella, 11.7ms\n",
            "Speed: 1.2ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 12.5ms\n",
            "Speed: 2.6ms preprocess, 12.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 11.5ms\n",
            "Speed: 1.2ms preprocess, 11.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 14.7ms\n",
            "Speed: 1.3ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 10.8ms\n",
            "Speed: 3.3ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 truck, 18.0ms\n",
            "Speed: 1.7ms preprocess, 18.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 10.7ms\n",
            "Speed: 2.9ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 truck, 15.8ms\n",
            "Speed: 1.1ms preprocess, 15.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 truck, 12.1ms\n",
            "Speed: 1.4ms preprocess, 12.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 406, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1554, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1206, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 517, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 510, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 493, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 647, in gen_wrapper\n",
            "    yield from f(*args, **kwargs)\n",
            "  File \"<ipython-input-43-6559f84e510a>\", line 46, in process_video\n",
            "    if object_count.count(id) == 0 :\n",
            "NameError: name 'object_count' is not defined\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://0845dcc7eed69bcc77.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://89a2d90d5a2b171aed.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "title = \"Object Detect Tracking and Counting\"\n",
        "with gr.Blocks(theme= gr.themes.Soft()) as io:\n",
        "    with gr.Tab(\"Video Tracking\") as record:\n",
        "        gr.Markdown(f\"<center><h1>{title}</h1></center>\")\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                input_image = gr.Video()\n",
        "\n",
        "            with gr.Column():\n",
        "                output_image = gr.Image()\n",
        "\n",
        "        with gr.Row():\n",
        "              total_count = gr.Textbox(label = \"Number of Object\")\n",
        "\n",
        "        with gr.Row():\n",
        "                input_button = gr.Button(\"Start Tracking\")\n",
        "                input_button.click(process_video, inputs=[input_image], outputs=[output_image, total_count])\n",
        "        with gr.Row():\n",
        "                input_button = gr.Button(\"Export Result\")\n",
        "                input_button.click(export_video, inputs= None, outputs = None)\n",
        "\n",
        "    with gr.Tab(\"Live Tracking\") as live:\n",
        "        gr.Markdown(f\"<center><h1>{title}</h1></center>\")\n",
        "        with gr.Row():\n",
        "          with gr.Column():\n",
        "             input_image = gr.Image(source='webcam', streaming=True)\n",
        "\n",
        "          with gr.Column():\n",
        "             output_image = gr.Image()\n",
        "        with gr.Row():\n",
        "              total_count = gr.Textbox(label = \"Number of Object\")\n",
        "        with gr.Row():\n",
        "             button  = gr.Button(\"Start Tracking\")\n",
        "             button.click(process_live, inputs=[input_image], outputs=[output_image])\n",
        "        with gr.Row():\n",
        "                input_button = gr.Button(\"Export Result\")\n",
        "                input_button.click(export_video, inputs= None, outputs = None)\n",
        "io.queue()\n",
        "io.launch(debug = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jta8mBvJmkST",
        "outputId": "033625f5-18b6-44d4-bd39-1764706dba0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xg2-a_UZfbHf",
        "outputId": "a33a2f0c-3d95-4a0b-ffbd-5c3865a12086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Track-And-Count-Object-using-YOLO'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 16 (delta 6), reused 10 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (16/16), 4.48 KiB | 4.48 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "/content/Track-And-Count-Object-using-YOLO\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.194-py3-none-any.whl (617 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.4/617.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (17.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.0.194\n",
            "Cloning into 'ByteTrack'...\n",
            "remote: Enumerating objects: 2007, done.\u001b[K\n",
            "remote: Total 2007 (delta 0), reused 0 (delta 0), pack-reused 2007\u001b[K\n",
            "Receiving objects: 100% (2007/2007), 79.60 MiB | 29.28 MiB/s, done.\n",
            "Resolving deltas: 100% (1141/1141), done.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mpython3: can't open file '/content/Track-And-Count-Object-using-YOLO/setup.py': [Errno 2] No such file or directory\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cython-bbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.22.4\n",
            "  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.12.3 requires numpy>=1.23.0, but you have numpy 1.22.4 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.22.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: supervision==0.1.0 in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from supervision==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from supervision==0.1.0) (4.8.0.76)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sankalpvarshney/Track-And-Count-Object-using-YOLO.git\n",
        "%cd Track-And-Count-Object-using-YOLO\n",
        "#!conda create --prefix ./env python=3.8 -y\n",
        "#conda activate ./env\n",
        "!pip install ultralytics\n",
        "!git clone https://github.com/ifzhang/ByteTrack.git\n",
        "!cd ByteTrack\n",
        "!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n",
        "!pip install -q -r requirements.txt\n",
        "!python setup.py -q develop\n",
        "!pip install -q cython-bbox\n",
        "!pip install -q onemetric\n",
        "!pip install -q loguru lap\n",
        "!pip install numpy==1.22.4\n",
        "!pip install supervision==0.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "nwnDbSTQpjkG",
        "outputId": "669990c2-8f12-4dc2-c828-64416991d331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-68.2.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install --upgrade setuptools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbFieWrOquiz",
        "outputId": "71d1a09d-f545-4c56-ac9e-23169c01660b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.41.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (68.2.2)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wheel setuptools pip --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zUcFkoAnsxf"
      },
      "outputs": [],
      "source": [
        "!pip3 install yolox --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B0DhfWQ9qUGK",
        "outputId": "d1778364-9c24-480e-fbf0-b2533b0c20eb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiQTb8H_qXUf",
        "outputId": "6606f2ac-2b2e-4368-8bbd-c5507e3696b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Track-And-Count-Object-using-YOLO\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Track-And-Count-Object-using-YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnyqq1kUrPeb",
        "outputId": "b8fb80a7-bfb6-4540-cce4-52faf6b5ea32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Track-And-Count-Object-using-YOLO/yolov8tracker.py\", line 1, in <module>\n",
            "    from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
            "ModuleNotFoundError: No module named 'yolox'\n"
          ]
        }
      ],
      "source": [
        "!python3 yolov8tracker.py -i \"/content/out.avi\" -o \"/content/out1.avi\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "A1tb3jTvf41W",
        "outputId": "f560168e-176f-4986-e1df-c83afa7090f7"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bac120630c6b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myolov8tracker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrackObject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrackObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/out.avi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/out1.avi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Track-And-Count-Object-using-YOLO/yolov8tracker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mByteTrack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_tracker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBYTETracker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTrack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0monemetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miou\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbox_iou_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msupervision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColorPalette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msupervision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Track-And-Count-Object-using-YOLO/ByteTrack/yolox/tracker/byte_tracker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from .matching import (merge_matches, _indices_to_matches,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mlinear_assignment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mious\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_distance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Track-And-Count-Object-using-YOLO/ByteTrack/yolox/tracker/matching.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcython_bbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbbox_overlaps\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbbox_ious\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkalman_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msrc/cython_bbox.pyx\u001b[0m in \u001b[0;36minit cython_bbox\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from yolov8tracker import TrackObject\n",
        "obj = TrackObject(\"/content/out.avi\",\"/content/out1.avi\")\n",
        "obj.process_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka1ZxynNiMtK",
        "outputId": "cb9e4a8c-083a-489d-975a-0b3a7cb21b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZCoWjPRiPFl",
        "outputId": "10b1c82f-d02a-4561-afa2-78b7d8a0274d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Object-tracking-and-counting-using-YOLOV8'...\n",
            "remote: Enumerating objects: 249, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 249 (delta 27), reused 15 (delta 4), pack-reused 198\u001b[K\n",
            "Receiving objects: 100% (249/249), 489.13 MiB | 31.55 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "Updating files: 100% (83/83), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mohamedamine99/Object-tracking-and-counting-using-YOLOV8.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eOQEfX9iSwh",
        "outputId": "ab837b93-b944-4483-e87e-67f133074db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Object-tracking-and-counting-using-YOLOV8\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Object-tracking-and-counting-using-YOLOV8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfH_yb7Pic-Z",
        "outputId": "deacdbbc-016a-4dd9-daf9-1967386dadd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting filterpy (from -r requirements.txt (line 1))\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/178.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m143.4/178.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.19.3)\n",
            "Requirement already satisfied: lap in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from filterpy->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from filterpy->-r requirements.txt (line 1)) (1.11.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from filterpy->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (2.31.5)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy->-r requirements.txt (line 1)) (1.16.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110459 sha256=1e9753d3739658ce4f27eddfc33f2935dd611c2f1bd35f477653d682f2d0d779\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXHB_X2liode",
        "outputId": "ca1d5704-5b59-468e-a1fd-09c84009e923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov8-live'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 29 (delta 13), reused 14 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (29/29), 6.18 KiB | 6.18 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SkalskiP/yolov8-live.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wEJcEzuvTum",
        "outputId": "50d1787d-132f-49ae-99c2-f0025c2298f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov8-live\n"
          ]
        }
      ],
      "source": [
        "%cd /content/yolov8-live"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCjrjHndwSN5",
        "outputId": "cef62d78-9827-4ad9-8ecd-d6cb19cb136a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The virtual environment was not created successfully because ensurepip is not\n",
            "available.  On Debian/Ubuntu systems, you need to install the python3-venv\n",
            "package using the following command.\n",
            "\n",
            "    apt install python3.10-venv\n",
            "\n",
            "You may need to use sudo with that command.  After installing the python3-venv\n",
            "package, recreate your virtual environment.\n",
            "\n",
            "Failing command: /content/yolov8-live/venv/bin/python3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 -m venv venv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19uGmUZxvcHu",
        "outputId": "6c1357a7-9936-42fb-de10-fa697d636b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.4/275.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYvqtUAZvmzT",
        "outputId": "7c09e4f4-12de-4fda-cf5c-d233a199b486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ WARN:0@6.324] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
            "[ERROR:0@6.325] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt to yolov8l.pt...\n",
            "100% 83.7M/83.7M [00:05<00:00, 16.4MB/s]\n",
            "2023-10-07 10:50:36.921264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-07 10:50:37.900021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Ultralytics YOLOv8.0.32 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov8-live/main.py\", line 81, in <module>\n",
            "    main()\n",
            "  File \"/content/yolov8-live/main.py\", line 58, in main\n",
            "    result = model(frame, agnostic_nms=True)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/model.py\", line 67, in __call__\n",
            "    return self.predict(source, stream, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/model.py\", line 147, in predict\n",
            "    return self.predictor.predict_cli(source=source) if is_cli else self.predictor(source=source, stream=stream)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/predictor.py\", line 114, in __call__\n",
            "    return list(self.stream_inference(source, model))  # merge list of Result into one\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/predictor.py\", line 142, in stream_inference\n",
            "    self.setup_source(source if source is not None else self.args.source)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/predictor.py\", line 124, in setup_source\n",
            "    self.dataset = load_inference_source(source=source,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/data/build.py\", line 183, in load_inference_source\n",
            "    dataset = LoadImages(source,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/data/dataloaders/stream_loaders.py\", line 187, in __init__\n",
            "    raise FileNotFoundError(f'{p} does not exist')\n",
            "FileNotFoundError: /content/yolov8-live/ultralytics/assets does not exist\n"
          ]
        }
      ],
      "source": [
        "!python3 main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsez35Af2MZV",
        "outputId": "7fef6803-0d29-4c05-a555-d38a00c641bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'YoLo-Nas_Object-Detection_Tracking'...\n",
            "remote: Enumerating objects: 125, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 125 (delta 16), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Receiving objects: 100% (125/125), 174.31 KiB | 901.00 KiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/spaces/ayoubkirouane/YoLo-Nas_Object-Detection_Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3YGbZoY2QEc",
        "outputId": "21b7ef33-fa0e-4350-ac21-b9f7a8f7db8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov8-live/YoLo-Nas_Object-Detection_Tracking\n"
          ]
        }
      ],
      "source": [
        "%cd /content/yolov8-live/YoLo-Nas_Object-Detection_Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MxSmMV_T2XC2",
        "outputId": "6432a5a1-0993-492b-e6ca-f2b91e07522a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting super-gradients==3.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading super_gradients-3.1.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filterpy==1.1.0 (from -r requirements.txt (line 2))\n",
            "  Downloading filterpy-1.1.0.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.8.0.76)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.10)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (4.66.1)\n",
            "Collecting boto3>=1.17.15 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading boto3-1.28.62-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (4.19.1)\n",
            "Collecting Deprecated>=1.2.11 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: scipy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (1.11.3)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (2.13.0)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (67.7.2)\n",
            "Collecting coverage~=5.3.1 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading coverage-5.3.1.tar.gz (684 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m684.5/684.5 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (0.15.2+cu118)\n",
            "Collecting sphinx~=4.0.2 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading Sphinx-4.0.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-rtd-theme (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading sphinx_rtd_theme-1.3.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics==0.8 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.6/408.6 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.2.0 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime==1.13.1 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading onnxruntime-1.13.1-cp310-cp310-manylinux_2_27_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx==1.13.0 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading onnx-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (9.4.0)\n",
            "Requirement already satisfied: pip-tools>=6.12.1 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (6.13.0)\n",
            "Collecting pyparsing==2.4.5 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading pyparsing-2.4.5-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.3.2 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Collecting pycocotools==2.0.6 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (3.20.3)\n",
            "Collecting treelib==1.6.1 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading treelib-1.6.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting termcolor==1.1.0 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: wheel>=0.38.0 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (0.41.2)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from super-gradients==3.1.3->-r requirements.txt (line 1)) (2.16.1)\n",
            "Collecting stringcase>=1.2.0 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy (from -r requirements.txt (line 4))\n",
            "  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading rapidfuzz-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json-tricks==3.16.1 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading json_tricks-3.16.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting onnx-simplifier<1.0,>=0.3.6 (from super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading onnx_simplifier-0.4.33-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.13.0->super-gradients==3.1.3->-r requirements.txt (line 1)) (4.5.0)\n",
            "Collecting coloredlogs (from onnxruntime==1.13.1->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.13.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.13.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.12)\n",
            "Collecting pyDeprecate==0.3.* (from torchmetrics==0.8->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from treelib==1.6.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (0.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.12.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 3)) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 3)) (17.0.2)\n",
            "Collecting botocore<1.32.0,>=1.31.62 (from boto3>=1.17.15->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading botocore-1.31.62-py3-none-any.whl (11.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.17.15->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3>=1.17.15->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated>=1.2.11->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.2.0->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->super-gradients==3.1.3->-r requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->super-gradients==3.1.3->-r requirements.txt (line 1)) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->super-gradients==3.1.3->-r requirements.txt (line 1)) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->super-gradients==3.1.3->-r requirements.txt (line 1)) (0.10.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients==3.1.3->-r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients==3.1.3->-r requirements.txt (line 1)) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->super-gradients==3.1.3->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf->super-gradients==3.1.3->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnx-simplifier<1.0,>=0.3.6->super-gradients==3.1.3->-r requirements.txt (line 1)) (13.6.0)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from pip-tools>=6.12.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.0.3)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.10/dist-packages (from pip-tools>=6.12.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.10/dist-packages (from pip-tools>=6.12.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (23.1.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.0.6)\n",
            "Collecting docutils<0.18,>=0.14 (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (2.13.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (3.0.0)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.13.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.62->boto3>=1.17.15->super-gradients==3.1.3->-r requirements.txt (line 1)) (2.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1)) (2023.7.22)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->pip-tools>=6.12.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->pip-tools>=6.12.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (2.0.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.13.1->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnx-simplifier<1.0,>=0.3.6->super-gradients==3.1.3->-r requirements.txt (line 1)) (3.0.0)\n",
            "INFO: pip is looking at multiple versions of sphinxcontrib-applehelp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sphinxcontrib-applehelp (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading sphinxcontrib_applehelp-1.0.6-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_applehelp-1.0.5-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of sphinxcontrib-devhelp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sphinxcontrib-devhelp (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading sphinxcontrib_devhelp-1.0.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_devhelp-1.0.3-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of sphinxcontrib-htmlhelp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sphinxcontrib-htmlhelp (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.3-py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_htmlhelp-2.0.2-py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of sphinxcontrib-qthelp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sphinxcontrib-qthelp (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading sphinxcontrib_qthelp-1.0.5-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_qthelp-1.0.4-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of sphinxcontrib-serializinghtml to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sphinxcontrib-serializinghtml (from sphinx~=4.0.2->super-gradients==3.1.3->-r requirements.txt (line 1))\n",
            "  Downloading sphinxcontrib_serializinghtml-1.1.8-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_serializinghtml-1.1.7-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_serializinghtml-1.1.6-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->onnx-simplifier<1.0,>=0.3.6->super-gradients==3.1.3->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->super-gradients==3.1.3->-r requirements.txt (line 1)) (3.2.2)\n",
            "Building wheels for collected packages: filterpy, pycocotools, termcolor, treelib, coverage, antlr4-python3-runtime, stringcase\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.1.0-py3-none-any.whl size=85422 sha256=ece1550f07c8d83b653a2b3515d5c607379cd541dbdd8628106a6e5d42cc81a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/26/1e/afde04eff5384bdd90a0c517b6fbe923fc2203c3200992509e\n",
            "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl size=376905 sha256=6e6b1e909f6ea3441983f7505b1f3635935d5603d9070851b83e4c6f8e2276a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/e6/f9/f87c8f8be098b51b616871315318329cae12cdb618f4caac93\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=6c4740188ca6186b2d55d65fb76b11e50a8e1374521044262498691d43df14fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "  Building wheel for treelib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for treelib: filename=treelib-1.6.1-py3-none-any.whl size=18368 sha256=ab6e1354fc7b0703a5e85fad0cce4be388a77f82d110df9986c45917eeb85101\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/72/8b/76569b82bf280a03c4e294c3b29ee2398217186369c427ed4b\n",
            "  Building wheel for coverage (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for coverage: filename=coverage-5.3.1-cp310-cp310-linux_x86_64.whl size=235260 sha256=772aaf0346af22177371eff004e4dd09c9406876600726266c74cb7c16c1a5ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/70/10/313be697f460d6024cfa94b7f0e22ffc1c53aab718fb4f42af\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=30d547263da8e016773ead0fc83ca188f38ad389bb98362738dc170542d94122\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3568 sha256=687fceacdae23b47f89b6f682a98914c36cb466384a4fd768528d76af9b6a339\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/ba/22/1a2d952a9ce8aa86e42fda41e2c87fdaf20e238c88bf8df013\n",
            "Successfully built filterpy pycocotools termcolor treelib coverage antlr4-python3-runtime stringcase\n",
            "Installing collected packages: termcolor, stringcase, json-tricks, einops, antlr4-python3-runtime, treelib, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, rapidfuzz, pyparsing, pyDeprecate, omegaconf, numpy, jmespath, humanfriendly, docutils, Deprecated, coverage, sphinx, onnx, hydra-core, coloredlogs, botocore, sphinxcontrib-jquery, s3transfer, onnxruntime, onnx-simplifier, sphinx-rtd-theme, pycocotools, filterpy, boto3, torchmetrics, super-gradients\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.3.0\n",
            "    Uninstalling termcolor-2.3.0:\n",
            "      Successfully uninstalled termcolor-2.3.0\n",
            "  Attempting uninstall: sphinxcontrib-serializinghtml\n",
            "    Found existing installation: sphinxcontrib-serializinghtml 1.1.9\n",
            "    Uninstalling sphinxcontrib-serializinghtml-1.1.9:\n",
            "      Successfully uninstalled sphinxcontrib-serializinghtml-1.1.9\n",
            "  Attempting uninstall: sphinxcontrib-qthelp\n",
            "    Found existing installation: sphinxcontrib-qthelp 1.0.6\n",
            "    Uninstalling sphinxcontrib-qthelp-1.0.6:\n",
            "      Successfully uninstalled sphinxcontrib-qthelp-1.0.6\n",
            "  Attempting uninstall: sphinxcontrib-htmlhelp\n",
            "    Found existing installation: sphinxcontrib-htmlhelp 2.0.4\n",
            "    Uninstalling sphinxcontrib-htmlhelp-2.0.4:\n",
            "      Successfully uninstalled sphinxcontrib-htmlhelp-2.0.4\n",
            "  Attempting uninstall: sphinxcontrib-devhelp\n",
            "    Found existing installation: sphinxcontrib-devhelp 1.0.5\n",
            "    Uninstalling sphinxcontrib-devhelp-1.0.5:\n",
            "      Successfully uninstalled sphinxcontrib-devhelp-1.0.5\n",
            "  Attempting uninstall: sphinxcontrib-applehelp\n",
            "    Found existing installation: sphinxcontrib-applehelp 1.0.7\n",
            "    Uninstalling sphinxcontrib-applehelp-1.0.7:\n",
            "      Successfully uninstalled sphinxcontrib-applehelp-1.0.7\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 5.0.2\n",
            "    Uninstalling Sphinx-5.0.2:\n",
            "      Successfully uninstalled Sphinx-5.0.2\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.7\n",
            "    Uninstalling pycocotools-2.0.7:\n",
            "      Successfully uninstalled pycocotools-2.0.7\n",
            "Successfully installed Deprecated-1.2.14 antlr4-python3-runtime-4.9.3 boto3-1.28.62 botocore-1.31.62 coloredlogs-15.0.1 coverage-5.3.1 docutils-0.17.1 einops-0.3.2 filterpy-1.1.0 humanfriendly-10.0 hydra-core-1.3.2 jmespath-1.0.1 json-tricks-3.16.1 numpy-1.23.0 omegaconf-2.3.0 onnx-1.13.0 onnx-simplifier-0.4.33 onnxruntime-1.13.1 pyDeprecate-0.3.2 pycocotools-2.0.6 pyparsing-2.4.5 rapidfuzz-3.3.1 s3transfer-0.7.0 sphinx-4.0.3 sphinx-rtd-theme-1.3.0 sphinxcontrib-applehelp-1.0.4 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.1 sphinxcontrib-jquery-4.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5 stringcase-1.2.0 super-gradients-3.1.3 termcolor-1.1.0 torchmetrics-0.8.0 treelib-1.6.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pydevd_plugins",
                  "pyparsing",
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGQXLHi22wak",
        "outputId": "23243531-ff96-4853-9570-ea6d6315233f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.27.2-py2.py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.0)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.6.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.1)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.37 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.27.2 validators-0.22.0 watchdog-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmoYfOKQ3Wrt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MOnHSxJPah0",
        "outputId": "e2fb338d-6962-49ed-81ad-713fee475145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Object-tracking-and-counting-using-YOLOV8'...\n",
            "remote: Enumerating objects: 249, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 249 (delta 27), reused 15 (delta 4), pack-reused 198\u001b[K\n",
            "Receiving objects: 100% (249/249), 489.13 MiB | 32.54 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "Updating files: 100% (83/83), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mohamedamine99/Object-tracking-and-counting-using-YOLOV8.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqCsGstRPlPb",
        "outputId": "6874d698-9ce7-464e-e977-f5aa0df79f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Object-tracking-and-counting-using-YOLOV8\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Object-tracking-and-counting-using-YOLOV8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEZ-y2QsPtEM",
        "outputId": "559164bb-8e56-4cdf-eec8-bc3ceca9d82e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting filterpy (from -r requirements.txt (line 1))\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.19.3)\n",
            "Collecting lap (from -r requirements.txt (line 3))\n",
            "  Downloading lap-0.4.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from filterpy->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from filterpy->-r requirements.txt (line 1)) (1.11.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from filterpy->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (2.31.5)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->filterpy->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy->-r requirements.txt (line 1)) (1.16.0)\n",
            "Building wheels for collected packages: filterpy, lap\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110459 sha256=d7ac50e1f209659ec092a83bbf319414cabce11ce79f6a3a79bfa5f586321a00\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lap: filename=lap-0.4.0-cp310-cp310-linux_x86_64.whl size=1628963 sha256=f3e46fba672d6317f08b7437feaa3ed73ba2e7be673d4578761b9f37da7de1ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/42/2e/9dfe19270eea279d79e84767ff0d7b8082c3bf776cad00e83d\n",
            "Successfully built filterpy lap\n",
            "Installing collected packages: lap, filterpy\n",
            "Successfully installed filterpy-1.4.5 lap-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24z6-28fQAfX",
        "outputId": "f218b126-bb4d-4840-bcf0-f7eba5d64257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/618.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/618.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.9/618.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uil4uvxkPyHo",
        "outputId": "1ac004bd-bd4a-4a82-db42-a6f68bd47121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K\r\u001b[2K\rUltralytics YOLOv8.0.195 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 27.4/78.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!python3 yolo_detect_and_count.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrK5hcxTQkQw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import sort\n",
        "\n",
        "import time\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class YOLOv8_ObjectDetector:\n",
        "    \"\"\"\n",
        "    A class for performing object detection on images and videos using YOLOv8.\n",
        "\n",
        "    Args:\n",
        "    ------------\n",
        "        model_file (str): Path to the YOLOv8 model file or yolo model variant name in ths format: [variant].pt\n",
        "        labels (list[str], optional): A list of class labels for the model. If None, uses the default labels from the model file.\n",
        "        classes (list[int], optional): The classes we want to detect, use it to exclude certain classes from being detected,\n",
        "            by default all classes in labels are detectable.\n",
        "        conf (float, optional): Minimum confidence threshold for object detection.\n",
        "        iou (float, optional): Minimum IOU threshold for non-max suppression.\n",
        "\n",
        "    Attributes:\n",
        "    --------------\n",
        "        classes (list[str]): A list of class labels for the model ( a Dict is also acceptable).\n",
        "        conf (float): Minimum confidence threshold for object detection.\n",
        "        iou (float): Minimum IOU threshold for non-max suppression.\n",
        "        model (YOLO): The YOLOv8 model used for object detection.\n",
        "        model_name (str): The name of the YOLOv8 model file (without the .pt extension).\n",
        "\n",
        "    Methods :\n",
        "    -------------\n",
        "        default_display: Returns a default display (ultralytics plot implementation) of the object detection results.\n",
        "        custom_display: Returns a custom display of the object detection results.\n",
        "        predict_video: Predicts objects in a video and saves the results to a file.\n",
        "        predict_img: Predicts objects in an image and returns the detection results.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_file = 'yolov8n.pt', labels= None, classes = None, conf = 0.25, iou = 0.45 ):\n",
        "\n",
        "        self.classes = classes\n",
        "        self.conf = conf\n",
        "        self.iou = iou\n",
        "\n",
        "        self.model = YOLO(model_file)\n",
        "        self.model_name = model_file.split('.')[0]\n",
        "        self.results = None\n",
        "\n",
        "        # if no labels are provided then use default COCO names\n",
        "        if labels == None:\n",
        "            self.labels = self.model.names\n",
        "        else:\n",
        "            self.labels = labels\n",
        "\n",
        "    def predict_img(self, img, verbose=True):\n",
        "        \"\"\"\n",
        "        Runs object detection on a single image.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            img (numpy.ndarray): Input image to perform object detection on.\n",
        "            verbose (bool): Whether to print detection details.\n",
        "\n",
        "        Returns:\n",
        "        -----------\n",
        "            'ultralytics.yolo.engine.results.Results': A YOLO results object that contains\n",
        "             details about detection results :\n",
        "                    - Class IDs\n",
        "                    - Bounding Boxes\n",
        "                    - Confidence score\n",
        "                    ...\n",
        "        (pls refer to https://docs.ultralytics.com/reference/results/#results-api-reference for results API reference)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Run the model on the input image with the given parameters\n",
        "        results = self.model(img, classes=self.classes, conf=self.conf, iou=self.iou, verbose=verbose)\n",
        "\n",
        "        # Save the original image and the results for further analysis if needed\n",
        "        self.orig_img = img\n",
        "        self.results = results[0]\n",
        "\n",
        "        # Return the detection results\n",
        "        return results[0]\n",
        "\n",
        "\n",
        "\n",
        "    def default_display(self, show_conf=True, line_width=None, font_size=None,\n",
        "                        font='Arial.ttf', pil=False, example='abc'):\n",
        "        \"\"\"\n",
        "        Displays the detected objects on the original input image.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        show_conf : bool, optional\n",
        "            Whether to show the confidence score of each detected object, by default True.\n",
        "        line_width : int, optional\n",
        "            The thickness of the bounding box line in pixels, by default None.\n",
        "        font_size : int, optional\n",
        "            The font size of the text label for each detected object, by default None.\n",
        "        font : str, optional\n",
        "            The font type of the text label for each detected object, by default 'Arial.ttf'.\n",
        "        pil : bool, optional\n",
        "            Whether to return a PIL Image object, by default False.\n",
        "        example : str, optional\n",
        "            A string to display on the example bounding box, by default 'abc'.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        numpy.ndarray or PIL Image\n",
        "            The original input image with the detected objects displayed as bounding boxes.\n",
        "            If `pil=True`, a PIL Image object is returned instead.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            If the input image has not been detected by calling the `predict_img()` method first.\n",
        "        \"\"\"\n",
        "        # Check if the `predict_img()` method has been called before displaying the detected objects\n",
        "        if self.results is None:\n",
        "            raise ValueError('No detected objects to display. Call predict_img() method first.')\n",
        "\n",
        "        # Call the plot() method of the `self.results` object to display the detected objects on the original image\n",
        "        display_img = self.results.plot(show_conf, line_width, font_size, font, pil, example)\n",
        "\n",
        "        # Return the displayed image\n",
        "        return display_img\n",
        "\n",
        "\n",
        "\n",
        "    def custom_display(self, colors, show_cls = True, show_conf = True):\n",
        "        \"\"\"\n",
        "        Custom display method that draws bounding boxes and labels on the original image,\n",
        "        with additional options for showing class and confidence information.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        colors : list\n",
        "            A list of tuples specifying the color of each class.\n",
        "        show_cls : bool, optional\n",
        "            Whether to show class information in the label text. Default is True.\n",
        "        show_conf : bool, optional\n",
        "            Whether to show confidence information in the label text. Default is True.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            The image with bounding boxes and labels drawn on it.\n",
        "        \"\"\"\n",
        "\n",
        "        img = self.orig_img\n",
        "        # calculate the bounding box thickness based on the image width and height\n",
        "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
        "\n",
        "        for box in self.results.boxes:\n",
        "            textString = \"\"\n",
        "\n",
        "            # Extract object class and confidence score\n",
        "            score = box.conf.item() * 100\n",
        "            class_id = int(box.cls.item())\n",
        "\n",
        "            x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
        "\n",
        "            # Print detection info\n",
        "            if show_cls:\n",
        "                textString += f\"{self.labels[class_id]}\"\n",
        "\n",
        "            if show_conf:\n",
        "                textString += f\" {score:,.2f}%\"\n",
        "\n",
        "            # Calculate font scale based on object size\n",
        "            font = cv2.FONT_HERSHEY_COMPLEX\n",
        "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
        "            fontThickness = 1\n",
        "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
        "\n",
        "            # Draw bounding box, a centroid and label on the image\n",
        "            img = cv2.rectangle(img, (x1,y1), (x2,y2), colors[class_id], bbx_thickness)\n",
        "            center_coordinates = ((x1 + x2)//2, (y1 + y2) // 2)\n",
        "\n",
        "            img =  cv2.circle(img, center_coordinates, 5 , (0,0,255), -1)\n",
        "\n",
        "             # If there are no details to show on the image\n",
        "            if textString != \"\":\n",
        "                if (y1 < textSize[1]):\n",
        "                    y1 = y1 + textSize[1]\n",
        "                else:\n",
        "                    y1 -= 2\n",
        "                # show the details text in a filled rectangle\n",
        "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), colors[class_id], cv2.FILLED)\n",
        "                img = cv2.putText(img, textString ,\n",
        "                    (x1, y1), font,\n",
        "                    fontScale,  (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "    def predict_video(self, video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args):\n",
        "        \"\"\"Runs object detection on each frame of a video and saves the output to a new video file.\n",
        "\n",
        "        Args:\n",
        "        ----------\n",
        "            video_path (str): The path to the input video file.\n",
        "            save_dir (str): The path to the directory where the output video file will be saved.\n",
        "            save_format (str, optional): The format for the output video file. Defaults to \"avi\".\n",
        "            display (str, optional): The type of display for the detection results. Defaults to 'custom'.\n",
        "            verbose (bool, optional): Whether to print information about the video file and output file. Defaults to True.\n",
        "            **display_args: Additional arguments to be passed to the display function.\n",
        "\n",
        "        Returns:\n",
        "        ------------\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Open the input video file\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Get the name of the input video file\n",
        "        vid_name = os.path.basename(video_path)\n",
        "\n",
        "        # Get the dimensions of each frame in the input video file\n",
        "        width = int(cap.get(3))  # get `width`\n",
        "        height = int(cap.get(4))  # get `height`\n",
        "\n",
        "        # Create the directory for the output video file if it does not already exist\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        # Set the name and path for the output video file\n",
        "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
        "        save_file = os.path.join(save_dir, save_name)\n",
        "\n",
        "        # Print information about the input and output video files if verbose is True\n",
        "        if verbose:\n",
        "            print(\"----------------------------\")\n",
        "            print(f\"DETECTING OBJECTS IN : {vid_name} : \")\n",
        "            print(f\"RESOLUTION : {width}x{height}\")\n",
        "            print('SAVING TO :' + save_file)\n",
        "\n",
        "        # Define an output VideoWriter object\n",
        "        out = cv2.VideoWriter(save_file,\n",
        "                              cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
        "                              30, (width, height))\n",
        "\n",
        "        # Check if the input video file was opened correctly\n",
        "        if not cap.isOpened():\n",
        "            print(\"Error opening video stream or file\")\n",
        "\n",
        "        # Read each frame of the input video file\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            # If the frame was not read successfully, break the loop\n",
        "            if not ret:\n",
        "                print(\"Error reading frame\")\n",
        "                break\n",
        "\n",
        "            # Run object detection on the frame and calculate FPS\n",
        "            beg = time.time()\n",
        "            results = self.predict_img(frame, verbose=False)\n",
        "            if results is None:\n",
        "                print('***********************************************')\n",
        "            fps = 1 / (time.time() - beg)\n",
        "\n",
        "            # Display the detection results\n",
        "            if display == 'default':\n",
        "                frame = self.default_display(**display_args)\n",
        "            elif display == 'custom':\n",
        "                frame == self.custom_display(**display_args)\n",
        "\n",
        "            # Display the FPS on the frame\n",
        "            frame = cv2.putText(frame, f\"FPS : {fps:,.2f}\",\n",
        "                                (5, 15), cv2.FONT_HERSHEY_COMPLEX,\n",
        "                                0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "            # Write the frame to the output video file\n",
        "            out.write(frame)\n",
        "\n",
        "            # Exit the loop if the 'q' button is pressed\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        # After the loop release the cap and video writer\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "class YOLOv8_ObjectCounter(YOLOv8_ObjectDetector):\n",
        "    \"\"\"\n",
        "    A class for counting objects in images or videos using the YOLOv8 Object Detection model.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    model_file : str\n",
        "        The filename of the YOLOv8 object detection model.\n",
        "    labels : list or None\n",
        "        The list of labels for the object detection model. If None, the labels will be loaded from the model file.\n",
        "    classes : list or None\n",
        "        The list of classes to detect. If None, all classes will be detected.\n",
        "    conf : float\n",
        "        The confidence threshold for object detection.\n",
        "    iou : float\n",
        "        The Intersection over Union (IoU) threshold for object detection.\n",
        "    track_max_age : int\n",
        "        The maximum age (in frames) of a track before it is deleted.\n",
        "    track_min_hits : int\n",
        "        The minimum number of hits required for a track to be considered valid.\n",
        "    track_iou_threshold : float\n",
        "        The IoU threshold for matching detections to existing tracks.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    predict_img(img, verbose=True)\n",
        "        Predicts objects in a single image and counts them.\n",
        "    predict_video(video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args)\n",
        "        Predicts objects in a video and counts them.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_file = 'yolov8n.pt', labels= None, classes = None, conf = 0.25, iou = 0.45,\n",
        "                 track_max_age = 45, track_min_hits= 15, track_iou_threshold = 0.3 ):\n",
        "\n",
        "        super().__init__(model_file , labels, classes, conf, iou)\n",
        "\n",
        "        self.track_max_age = track_max_age\n",
        "        self.track_min_hits = track_min_hits\n",
        "        self.track_iou_threshold = track_iou_threshold\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict_video(self, video_path, save_dir, save_format = \"avi\",\n",
        "                      display = 'custom', verbose = True, **display_args):\n",
        "\n",
        "        \"\"\"\n",
        "    Runs object detection on a video file and saves the output as a new video file.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video file.\n",
        "        save_dir (str): Path to the directory where the output video file will be saved.\n",
        "        save_format (str, optional): Format of the output video file. Defaults to \"avi\".\n",
        "        display (str, optional): Type of display to use for object detection results. Options are \"default\" or \"custom\".\n",
        "                                Defaults to \"custom\".\n",
        "        verbose (bool, optional): If True, prints information about the input and output video files. Defaults to True.\n",
        "        **display_args (dict, optional): Additional arguments to pass to the display function.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        # Get video name\n",
        "        vid_name = os.path.basename(video_path)\n",
        "\n",
        "\n",
        "        # Get frame dimensions and print information about input video file\n",
        "        width  = int(cap.get(3) )  # get `width`\n",
        "        height = int(cap.get(4) )  # get `height`\n",
        "\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
        "        save_file = os.path.join(save_dir, save_name)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"----------------------------\")\n",
        "            print(f\"DETECTING OBJECTS IN : {vid_name} : \")\n",
        "            print(f\"RESOLUTION : {width}x{height}\")\n",
        "            print('SAVING TO :' + save_file)\n",
        "\n",
        "        # define an output VideoWriter  object\n",
        "        out = cv2.VideoWriter(save_file,\n",
        "                            cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
        "                            30,(width,height))\n",
        "\n",
        "        # Check if the video is opened correctly\n",
        "        if not cap.isOpened():\n",
        "            print(\"Error opening video stream or file\")\n",
        "\n",
        "        # Initialize object tracker\n",
        "        tracker = sort.Sort(max_age = self.track_max_age, min_hits= self.track_min_hits ,\n",
        "                            iou_threshold = self.track_iou_threshold)\n",
        "\n",
        "        # Initialize variables for object counting\n",
        "        totalCount = []\n",
        "        currentArray = np.empty((0, 5))\n",
        "\n",
        "\n",
        "        # Read the video frames\n",
        "        while cap.isOpened():\n",
        "\n",
        "            detections = np.empty((0, 5))\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            # If the frame was not read successfully, break the loop\n",
        "            if not ret:\n",
        "                print(\"Error reading frame\")\n",
        "                break\n",
        "\n",
        "            # Run object detection on the frame and calculate FPS\n",
        "            beg = time.time()\n",
        "            results = self.predict_img(frame, verbose = False)\n",
        "            if results == None:\n",
        "                print('***********************************************')\n",
        "            fps = 1 / (time.time() - beg)\n",
        "            for box in results.boxes:\n",
        "                score = box.conf.item() * 100\n",
        "                class_id = int(box.cls.item())\n",
        "\n",
        "                x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
        "\n",
        "                currentArray = np.array([x1, y1, x2, y2, score])\n",
        "                detections = np.vstack((detections, currentArray))\n",
        "\n",
        "            # Update object tracker\n",
        "            resultsTracker = tracker.update(detections)\n",
        "            for result in resultsTracker:\n",
        "                #print(type(result))\n",
        "\n",
        "                # Get the tracker results\n",
        "                x1, y1, x2, y2, id = result\n",
        "                x1, y1, x2, y2, id = int(x1), int(y1), int(x2), int(y2), int(id)\n",
        "                #print(result)\n",
        "\n",
        "                # Display current objects IDs\n",
        "                w, h = x2 - x1, y2 - y1\n",
        "                cx, cy = x1 + w // 2, y1 + h // 2\n",
        "                id_txt = f\"ID: {str(id)}\"\n",
        "                cv2.putText(frame, id_txt, (cx, cy), 4, 0.5, (0, 0, 255), 1)\n",
        "\n",
        "                # if we haven't seen aprticular object ID before, register it in a list\n",
        "                if totalCount.count(id) == 0:\n",
        "                    totalCount.append(id)\n",
        "\n",
        "            # Display detection results\n",
        "            if display == 'default':\n",
        "                frame = self.default_display(**display_args)\n",
        "\n",
        "            elif display == 'custom':\n",
        "                frame == self.custom_display( **display_args)\n",
        "\n",
        "            # Display FPS on frame\n",
        "            frame = cv2.putText(frame,f\"FPS : {fps:,.2f}\" ,\n",
        "                                (5,55), cv2.FONT_HERSHEY_COMPLEX,\n",
        "                            0.5,  (0,255,255), 1, cv2.LINE_AA)\n",
        "\n",
        "            # Display Counting results\n",
        "            count_txt = f\"TOTAL COUNT : {len(totalCount)}\"\n",
        "            frame = cv2.putText(frame, count_txt, (5,45), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 0, 255), 2)\n",
        "\n",
        "\n",
        "            # append frame to the video file\n",
        "            out.write(frame)\n",
        "\n",
        "            # the 'q' button is set as the\n",
        "            # quitting button you may use any\n",
        "            # desired button of your choice\n",
        "\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        # After the loop release the cap\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(len(totalCount))\n",
        "        print(totalCount)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30c06c90e5914378bdc4e5d1a9bf456b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "349dffa168ef4ce4bf0822f0e31d75ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6fa9af8e8d1440ab17a00ef914bf561",
            "placeholder": "​",
            "style": "IPY_MODEL_d7ac195132624d91b7c7fa75db1673de",
            "value": "100%"
          }
        },
        "360d8b86050b40ed8015437643a3cc15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c50c42e25e80435aa350d21ad54e5335",
            "placeholder": "​",
            "style": "IPY_MODEL_c3853113ebab45b786de8638fa0ef146",
            "value": " 317/318 [00:08&lt;00:00, 32.35it/s]"
          }
        },
        "4c38c34c12694b6886f430451b15eb46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30c06c90e5914378bdc4e5d1a9bf456b",
            "max": 318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff2da230896347ed87e9e253e2c7fe38",
            "value": 317
          }
        },
        "5daefe68fc3b4669ab04d2291e9e344e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ffcb703d91542d8b23436c261a4c08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_349dffa168ef4ce4bf0822f0e31d75ce",
              "IPY_MODEL_4c38c34c12694b6886f430451b15eb46",
              "IPY_MODEL_360d8b86050b40ed8015437643a3cc15"
            ],
            "layout": "IPY_MODEL_5daefe68fc3b4669ab04d2291e9e344e"
          }
        },
        "c3853113ebab45b786de8638fa0ef146": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c50c42e25e80435aa350d21ad54e5335": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6fa9af8e8d1440ab17a00ef914bf561": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7ac195132624d91b7c7fa75db1673de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff2da230896347ed87e9e253e2c7fe38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
